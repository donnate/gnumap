My SLURM_ARRAY_TASK_ID:  15
My SLURM_ARRAY_JOB_ID:  4522289
DGI
Cora
result file is 4522289_15
Found it
Dataset: Cora():
======================
Number of graphs: 1
Number of features: 1433
Number of classes: 7
Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])
==============================================================
Number of nodes: 2708
Number of edges: 10556
Average node degree: 3.90
Number of training nodes: 140
Training node label rate: 0.05
Contains isolated nodes: False
Contains self-loops: False
Is undirected: True
/scratch/midway3/cdonnat/gnumap/experiments/results/DGI_results_Cora_4522289_15.csv
=== train DGI model ===
Epoch=000, loss=0.6931
Epoch=001, loss=0.6930
Epoch=002, loss=0.6930
Epoch=003, loss=0.6930
Epoch=004, loss=0.6929
Epoch=005, loss=0.6929
Epoch=006, loss=0.6929
Epoch=007, loss=0.6928
Epoch=008, loss=0.6928
Epoch=009, loss=0.6927
Epoch=010, loss=0.6926
Epoch=011, loss=0.6926
Epoch=012, loss=0.6925
Epoch=013, loss=0.6925
Epoch=014, loss=0.6924
Epoch=015, loss=0.6923
Epoch=016, loss=0.6922
Epoch=017, loss=0.6922
Epoch=018, loss=0.6921
Epoch=019, loss=0.6920
Epoch=020, loss=0.6918
Epoch=021, loss=0.6918
Epoch=022, loss=0.6916
Epoch=023, loss=0.6915
Epoch=024, loss=0.6914
Epoch=025, loss=0.6912
Epoch=026, loss=0.6910
Epoch=027, loss=0.6909
Epoch=028, loss=0.6907
Epoch=029, loss=0.6905
Epoch=030, loss=0.6904
Epoch=031, loss=0.6901
Epoch=032, loss=0.6900
Epoch=033, loss=0.6897
Epoch=034, loss=0.6895
Epoch=035, loss=0.6893
Epoch=036, loss=0.6889
Epoch=037, loss=0.6886
Epoch=038, loss=0.6882
Epoch=039, loss=0.6881
Epoch=040, loss=0.6878
Epoch=041, loss=0.6874
Epoch=042, loss=0.6870
Epoch=043, loss=0.6867
Epoch=044, loss=0.6862
Epoch=045, loss=0.6858
Epoch=046, loss=0.6855
Epoch=047, loss=0.6850
Epoch=048, loss=0.6844
Epoch=049, loss=0.6841
Epoch=050, loss=0.6836
Epoch=051, loss=0.6830
Epoch=052, loss=0.6824
Epoch=053, loss=0.6820
Epoch=054, loss=0.6812
Epoch=055, loss=0.6803
Epoch=056, loss=0.6801
Epoch=057, loss=0.6791
Epoch=058, loss=0.6785
Epoch=059, loss=0.6778
Epoch=060, loss=0.6767
Epoch=061, loss=0.6762
Epoch=062, loss=0.6753
Epoch=063, loss=0.6748
Epoch=064, loss=0.6734
Epoch=065, loss=0.6731
Epoch=066, loss=0.6718
Epoch=067, loss=0.6706
Epoch=068, loss=0.6696
Epoch=069, loss=0.6685
Epoch=070, loss=0.6673
Epoch=071, loss=0.6661
Epoch=072, loss=0.6652
Epoch=073, loss=0.6639
Epoch=074, loss=0.6621
Epoch=075, loss=0.6614
Epoch=076, loss=0.6597
Epoch=077, loss=0.6594
Epoch=078, loss=0.6573
Epoch=079, loss=0.6560
Epoch=080, loss=0.6548
Epoch=081, loss=0.6533
Epoch=082, loss=0.6521
Epoch=083, loss=0.6505
Epoch=084, loss=0.6474
Epoch=085, loss=0.6460
Epoch=086, loss=0.6452
Epoch=087, loss=0.6439
Epoch=088, loss=0.6412
Epoch=089, loss=0.6391
Epoch=090, loss=0.6375
Epoch=091, loss=0.6356
Epoch=092, loss=0.6337
Epoch=093, loss=0.6314
Epoch=094, loss=0.6311
Epoch=095, loss=0.6282
Epoch=096, loss=0.6264
Epoch=097, loss=0.6241
Epoch=098, loss=0.6210
Epoch=099, loss=0.6205
Epoch=100, loss=0.6165
Epoch=101, loss=0.6162
Epoch=102, loss=0.6129
Epoch=103, loss=0.6100
Epoch=104, loss=0.6070
Epoch=105, loss=0.6059
Epoch=106, loss=0.6021
Epoch=107, loss=0.6006
Epoch=108, loss=0.5985
Epoch=109, loss=0.5950
Epoch=110, loss=0.5938
Epoch=111, loss=0.5898
Epoch=112, loss=0.5870
Epoch=113, loss=0.5839
Epoch=114, loss=0.5819
Epoch=115, loss=0.5789
Epoch=116, loss=0.5768
Epoch=117, loss=0.5728
Epoch=118, loss=0.5707
Epoch=119, loss=0.5682
Epoch=120, loss=0.5639
Epoch=121, loss=0.5622
Epoch=122, loss=0.5585
Epoch=123, loss=0.5545
Epoch=124, loss=0.5506
Epoch=125, loss=0.5491
Epoch=126, loss=0.5471
Epoch=127, loss=0.5409
Epoch=128, loss=0.5369
Epoch=129, loss=0.5345
Epoch=130, loss=0.5320
Epoch=131, loss=0.5259
Epoch=132, loss=0.5217
Epoch=133, loss=0.5178
Epoch=134, loss=0.5165
Epoch=135, loss=0.5107
Epoch=136, loss=0.5104
Epoch=137, loss=0.5040
Epoch=138, loss=0.5038
Epoch=139, loss=0.4972
Epoch=140, loss=0.4963
Epoch=141, loss=0.4898
Epoch=142, loss=0.4880
Epoch=143, loss=0.4820
Epoch=144, loss=0.4763
Epoch=145, loss=0.4723
Epoch=146, loss=0.4665
Epoch=147, loss=0.4672
Epoch=148, loss=0.4617
Epoch=149, loss=0.4591
Epoch=150, loss=0.4509
Epoch=151, loss=0.4492
Epoch=152, loss=0.4451
Epoch=153, loss=0.4415
Epoch=154, loss=0.4384
Epoch=155, loss=0.4306
Epoch=156, loss=0.4294
Epoch=157, loss=0.4280
Epoch=158, loss=0.4231
Epoch=159, loss=0.4167
Epoch=160, loss=0.4136
Epoch=161, loss=0.4079
Epoch=162, loss=0.4041
Epoch=163, loss=0.4038
Epoch=164, loss=0.3929
Epoch=165, loss=0.3963
Epoch=166, loss=0.3874
Epoch=167, loss=0.3829
Epoch=168, loss=0.3770
Epoch=169, loss=0.3751
Epoch=170, loss=0.3745
Epoch=171, loss=0.3674
Epoch=172, loss=0.3637
Epoch=173, loss=0.3550
Epoch=174, loss=0.3582
Epoch=175, loss=0.3516
Epoch=176, loss=0.3484
Epoch=177, loss=0.3447
Epoch=178, loss=0.3424
Epoch=179, loss=0.3338
Epoch=180, loss=0.3352
Epoch=181, loss=0.3281
Epoch=182, loss=0.3241
Epoch=183, loss=0.3245
Epoch=184, loss=0.3209
Epoch=185, loss=0.3148
Epoch=186, loss=0.3123
Epoch=187, loss=0.3080
Epoch=188, loss=0.3101
Epoch=189, loss=0.3046
Epoch=190, loss=0.2945
Epoch=191, loss=0.2973
Epoch=192, loss=0.2883
Epoch=193, loss=0.2883
Epoch=194, loss=0.2845
Epoch=195, loss=0.2882
Epoch=196, loss=0.2823
Epoch=197, loss=0.2745
Epoch=198, loss=0.2735
Epoch=199, loss=0.2745
Epoch=200, loss=0.2681
Epoch=201, loss=0.2638
Epoch=202, loss=0.2661
Epoch=203, loss=0.2528
Epoch=204, loss=0.2581
Epoch=205, loss=0.2544
Epoch=206, loss=0.2498
Epoch=207, loss=0.2472
Epoch=208, loss=0.2464
Epoch=209, loss=0.2440
Epoch=210, loss=0.2411
Epoch=211, loss=0.2464
Epoch=212, loss=0.2324
Epoch=213, loss=0.2321
Epoch=214, loss=0.2338
Epoch=215, loss=0.2314
Epoch=216, loss=0.2321
Epoch=217, loss=0.2262
Epoch=218, loss=0.2223
Epoch=219, loss=0.2139
Epoch=220, loss=0.2162
Epoch=221, loss=0.2156
Epoch=222, loss=0.2149
Epoch=223, loss=0.2187
Epoch=224, loss=0.2098
Epoch=225, loss=0.2045
Epoch=226, loss=0.2109
Epoch=227, loss=0.2060
Epoch=228, loss=0.1994
Epoch=229, loss=0.1977
Epoch=230, loss=0.1936
Epoch=231, loss=0.1963
Epoch=232, loss=0.1954
Epoch=233, loss=0.1880
Epoch=234, loss=0.1943
Epoch=235, loss=0.1877
Epoch=236, loss=0.1886
Epoch=237, loss=0.1833
Epoch=238, loss=0.1830
Epoch=239, loss=0.1902
Epoch=240, loss=0.1765
Epoch=241, loss=0.1691
Epoch=242, loss=0.1757
Epoch=243, loss=0.1707
Epoch=244, loss=0.1787
Epoch=245, loss=0.1721
Epoch=246, loss=0.1742
Epoch=247, loss=0.1655
Epoch=248, loss=0.1707
Epoch=249, loss=0.1792
Epoch=250, loss=0.1704
Epoch=251, loss=0.1638
Epoch=252, loss=0.1594
Epoch=253, loss=0.1648
Epoch=254, loss=0.1579
Epoch=255, loss=0.1525
Epoch=256, loss=0.1573
Epoch=257, loss=0.1558
Epoch=258, loss=0.1524
Epoch=259, loss=0.1532
Epoch=260, loss=0.1607
Epoch=261, loss=0.1489
Epoch=262, loss=0.1475
Epoch=263, loss=0.1562
Epoch=264, loss=0.1463
Epoch=265, loss=0.1525
Epoch=266, loss=0.1480
Epoch=267, loss=0.1434
Epoch=268, loss=0.1460
Epoch=269, loss=0.1449
Epoch=270, loss=0.1433
Epoch=271, loss=0.1384
Epoch=272, loss=0.1363
Epoch=273, loss=0.1394
Epoch=274, loss=0.1382
Epoch=275, loss=0.1383
Epoch=276, loss=0.1308
Epoch=277, loss=0.1325
Epoch=278, loss=0.1412
Epoch=279, loss=0.1403
Epoch=280, loss=0.1293
Epoch=281, loss=0.1378
Epoch=282, loss=0.1377
Epoch=283, loss=0.1318
Epoch=284, loss=0.1229
Epoch=285, loss=0.1316
Epoch=286, loss=0.1227
Epoch=287, loss=0.1286
Epoch=288, loss=0.1228
Epoch=289, loss=0.1219
Epoch=290, loss=0.1166
Epoch=291, loss=0.1198
Epoch=292, loss=0.1267
Epoch=293, loss=0.1247
Epoch=294, loss=0.1232
Epoch=295, loss=0.1205
Epoch=296, loss=0.1154
Epoch=297, loss=0.1208
Epoch=298, loss=0.1195
Epoch=299, loss=0.1144
Epoch=300, loss=0.1150
Epoch=301, loss=0.1209
Epoch=302, loss=0.1082
Epoch=303, loss=0.1115
Epoch=304, loss=0.1146
Epoch=305, loss=0.1135
Epoch=306, loss=0.1180
Epoch=307, loss=0.1108
Epoch=308, loss=0.1201
Epoch=309, loss=0.1107
Epoch=310, loss=0.1118
Epoch=311, loss=0.1091
Epoch=312, loss=0.1124
Epoch=313, loss=0.1138
Epoch=314, loss=0.1110
Epoch=315, loss=0.1083
Epoch=316, loss=0.1048
Epoch=317, loss=0.1065
Epoch=318, loss=0.1019
Epoch=319, loss=0.1030
Epoch=320, loss=0.1038
Epoch=321, loss=0.1066
Epoch=322, loss=0.1094
Epoch=323, loss=0.1070
Epoch=324, loss=0.1035
Epoch=325, loss=0.1053
Epoch=326, loss=0.1012
Epoch=327, loss=0.0993
Epoch=328, loss=0.1061
Epoch=329, loss=0.1025
Epoch=330, loss=0.0995
Epoch=331, loss=0.0995
Epoch=332, loss=0.0909
Epoch=333, loss=0.1006
Epoch=334, loss=0.0986
Epoch=335, loss=0.0960
Epoch=336, loss=0.1012
Epoch=337, loss=0.0975
Epoch=338, loss=0.0970
Epoch=339, loss=0.0962
Epoch=340, loss=0.1010
Epoch=341, loss=0.0913
Epoch=342, loss=0.0913
Epoch=343, loss=0.1010
Epoch=344, loss=0.0989
Epoch=345, loss=0.0908
Epoch=346, loss=0.0903
Epoch=347, loss=0.0959
Epoch=348, loss=0.0982
Epoch=349, loss=0.0917
Epoch=350, loss=0.0955
Epoch=351, loss=0.0852
Epoch=352, loss=0.0899
Epoch=353, loss=0.0922
Epoch=354, loss=0.0818
Epoch=355, loss=0.0847
Epoch=356, loss=0.0941
Epoch=357, loss=0.0973
Epoch=358, loss=0.0881
Epoch=359, loss=0.0946
Epoch=360, loss=0.0933
Epoch=361, loss=0.0810
Epoch=362, loss=0.0806
Epoch=363, loss=0.0818
Epoch=364, loss=0.0898
Epoch=365, loss=0.0910
Epoch=366, loss=0.0771
Epoch=367, loss=0.0823
Epoch=368, loss=0.0920
Epoch=369, loss=0.0820
Epoch=370, loss=0.0883
Epoch=371, loss=0.0853
Epoch=372, loss=0.0854
Epoch=373, loss=0.0775
Epoch=374, loss=0.0843
Epoch=375, loss=0.0763
Epoch=376, loss=0.0773
Epoch=377, loss=0.0803
Epoch=378, loss=0.0753
Epoch=379, loss=0.0838
Epoch=380, loss=0.0770
Epoch=381, loss=0.0803
Epoch=382, loss=0.0731
Epoch=383, loss=0.0768
Epoch=384, loss=0.0827
Epoch=385, loss=0.0804
Epoch=386, loss=0.0857
Epoch=387, loss=0.0856
Epoch=388, loss=0.0739
Epoch=389, loss=0.0781
Epoch=390, loss=0.0732
Epoch=391, loss=0.0727
Epoch=392, loss=0.0922
Epoch=393, loss=0.0768
Epoch=394, loss=0.0747
Epoch=395, loss=0.0733
Epoch=396, loss=0.0835
Epoch=397, loss=0.0742
Epoch=398, loss=0.0728
Epoch=399, loss=0.0768
Epoch=400, loss=0.0764
Epoch=401, loss=0.0791
Epoch=402, loss=0.0755
Epoch=403, loss=0.0751
Epoch=404, loss=0.0694
Epoch=405, loss=0.0636
Epoch=406, loss=0.0717
Epoch=407, loss=0.0772
Epoch=408, loss=0.0683
Epoch=409, loss=0.0745
Epoch=410, loss=0.0681
Epoch=411, loss=0.0706
Epoch=412, loss=0.0719
Epoch=413, loss=0.0704
Epoch=414, loss=0.0752
Epoch=415, loss=0.0690
Epoch=416, loss=0.0687
Epoch=417, loss=0.0651
Epoch=418, loss=0.0684
Epoch=419, loss=0.0710
Epoch=420, loss=0.0691
Epoch=421, loss=0.0668
Epoch=422, loss=0.0659
Epoch=423, loss=0.0705
Epoch=424, loss=0.0674
Epoch=425, loss=0.0593
Epoch=426, loss=0.0700
Epoch=427, loss=0.0622
Epoch=428, loss=0.0562
Epoch=429, loss=0.0630
Epoch=430, loss=0.0596
Epoch=431, loss=0.0696
Epoch=432, loss=0.0777
Epoch=433, loss=0.0616
Epoch=434, loss=0.0608
Epoch=435, loss=0.0627
Epoch=436, loss=0.0740
Epoch=437, loss=0.0663
Epoch=438, loss=0.0634
Epoch=439, loss=0.0663
Epoch=440, loss=0.0645
Epoch=441, loss=0.0705
Epoch=442, loss=0.0652
Epoch=443, loss=0.0670
Epoch=444, loss=0.0624
Epoch=445, loss=0.0651
Epoch=446, loss=0.0659
Epoch=447, loss=0.0619
Epoch=448, loss=0.0639
Early stopping!
Loading 428th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7037+-0.0269, F1Ma=0.6377+-0.0412, acc=0.7037+-0.0269
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.765483307040204, 0.7749861751976163, 0.7759146506126476, 0.780777803196709, 0.8142856955528259, 0.7160000205039978, 0.6934235692024231, 0.8142856955528259, 0.7120000123977661, 0.6832688450813293, 0.7036921881072677, 0.026900291239213944, 0.6376950336999908, 0.04124841151711339, 0.7036921881072677, 0.026900291239213912]]
=== train DGI model ===
Epoch=000, loss=0.6931
Epoch=001, loss=0.6930
Epoch=002, loss=0.6929
Epoch=003, loss=0.6929
Epoch=004, loss=0.6928
Epoch=005, loss=0.6927
Epoch=006, loss=0.6926
Epoch=007, loss=0.6925
Epoch=008, loss=0.6924
Epoch=009, loss=0.6922
Epoch=010, loss=0.6920
Epoch=011, loss=0.6918
Epoch=012, loss=0.6916
Epoch=013, loss=0.6914
Epoch=014, loss=0.6910
Epoch=015, loss=0.6908
Epoch=016, loss=0.6905
Epoch=017, loss=0.6901
Epoch=018, loss=0.6897
Epoch=019, loss=0.6893
Epoch=020, loss=0.6889
Epoch=021, loss=0.6884
Epoch=022, loss=0.6878
Epoch=023, loss=0.6872
Epoch=024, loss=0.6865
Epoch=025, loss=0.6858
Epoch=026, loss=0.6851
Epoch=027, loss=0.6842
Epoch=028, loss=0.6834
Epoch=029, loss=0.6824
Epoch=030, loss=0.6812
Epoch=031, loss=0.6805
Epoch=032, loss=0.6792
Epoch=033, loss=0.6779
Epoch=034, loss=0.6764
Epoch=035, loss=0.6752
Epoch=036, loss=0.6736
Epoch=037, loss=0.6718
Epoch=038, loss=0.6705
Epoch=039, loss=0.6685
Epoch=040, loss=0.6663
Epoch=041, loss=0.6641
Epoch=042, loss=0.6622
Epoch=043, loss=0.6602
Epoch=044, loss=0.6580
Epoch=045, loss=0.6546
Epoch=046, loss=0.6522
Epoch=047, loss=0.6499
Epoch=048, loss=0.6468
Epoch=049, loss=0.6433
Epoch=050, loss=0.6405
Epoch=051, loss=0.6365
Epoch=052, loss=0.6325
Epoch=053, loss=0.6291
Epoch=054, loss=0.6263
Epoch=055, loss=0.6219
Epoch=056, loss=0.6168
Epoch=057, loss=0.6128
Epoch=058, loss=0.6094
Epoch=059, loss=0.6044
Epoch=060, loss=0.6000
Epoch=061, loss=0.5940
Epoch=062, loss=0.5907
Epoch=063, loss=0.5855
Epoch=064, loss=0.5787
Epoch=065, loss=0.5740
Epoch=066, loss=0.5651
Epoch=067, loss=0.5632
Epoch=068, loss=0.5551
Epoch=069, loss=0.5506
Epoch=070, loss=0.5430
Epoch=071, loss=0.5364
Epoch=072, loss=0.5318
Epoch=073, loss=0.5240
Epoch=074, loss=0.5175
Epoch=075, loss=0.5088
Epoch=076, loss=0.5026
Epoch=077, loss=0.4941
Epoch=078, loss=0.4896
Epoch=079, loss=0.4841
Epoch=080, loss=0.4729
Epoch=081, loss=0.4699
Epoch=082, loss=0.4580
Epoch=083, loss=0.4526
Epoch=084, loss=0.4436
Epoch=085, loss=0.4380
Epoch=086, loss=0.4293
Epoch=087, loss=0.4174
Epoch=088, loss=0.4121
Epoch=089, loss=0.4046
Epoch=090, loss=0.4000
Epoch=091, loss=0.3914
Epoch=092, loss=0.3867
Epoch=093, loss=0.3804
Epoch=094, loss=0.3711
Epoch=095, loss=0.3569
Epoch=096, loss=0.3576
Epoch=097, loss=0.3483
Epoch=098, loss=0.3374
Epoch=099, loss=0.3295
Epoch=100, loss=0.3252
Epoch=101, loss=0.3233
Epoch=102, loss=0.3165
Epoch=103, loss=0.3113
Epoch=104, loss=0.3030
Epoch=105, loss=0.2919
Epoch=106, loss=0.2891
Epoch=107, loss=0.2800
Epoch=108, loss=0.2768
Epoch=109, loss=0.2762
Epoch=110, loss=0.2704
Epoch=111, loss=0.2613
Epoch=112, loss=0.2562
Epoch=113, loss=0.2459
Epoch=114, loss=0.2425
Epoch=115, loss=0.2406
Epoch=116, loss=0.2303
Epoch=117, loss=0.2310
Epoch=118, loss=0.2224
Epoch=119, loss=0.2188
Epoch=120, loss=0.2229
Epoch=121, loss=0.2049
Epoch=122, loss=0.2103
Epoch=123, loss=0.2026
Epoch=124, loss=0.1950
Epoch=125, loss=0.1986
Epoch=126, loss=0.1853
Epoch=127, loss=0.1986
Epoch=128, loss=0.1754
Epoch=129, loss=0.1936
Epoch=130, loss=0.1797
Epoch=131, loss=0.1774
Epoch=132, loss=0.1765
Epoch=133, loss=0.1725
Epoch=134, loss=0.1730
Epoch=135, loss=0.1659
Epoch=136, loss=0.1579
Epoch=137, loss=0.1594
Epoch=138, loss=0.1541
Epoch=139, loss=0.1525
Epoch=140, loss=0.1597
Epoch=141, loss=0.1502
Epoch=142, loss=0.1394
Epoch=143, loss=0.1394
Epoch=144, loss=0.1424
Epoch=145, loss=0.1515
Epoch=146, loss=0.1426
Epoch=147, loss=0.1313
Epoch=148, loss=0.1402
Epoch=149, loss=0.1380
Epoch=150, loss=0.1317
Epoch=151, loss=0.1393
Epoch=152, loss=0.1281
Epoch=153, loss=0.1264
Epoch=154, loss=0.1182
Epoch=155, loss=0.1298
Epoch=156, loss=0.1182
Epoch=157, loss=0.1209
Epoch=158, loss=0.1114
Epoch=159, loss=0.1234
Epoch=160, loss=0.1165
Epoch=161, loss=0.1156
Epoch=162, loss=0.1136
Epoch=163, loss=0.1055
Epoch=164, loss=0.1121
Epoch=165, loss=0.1102
Epoch=166, loss=0.1107
Epoch=167, loss=0.1086
Epoch=168, loss=0.0991
Epoch=169, loss=0.1038
Epoch=170, loss=0.1124
Epoch=171, loss=0.1005
Epoch=172, loss=0.1071
Epoch=173, loss=0.0982
Epoch=174, loss=0.1037
Epoch=175, loss=0.1064
Epoch=176, loss=0.1008
Epoch=177, loss=0.0962
Epoch=178, loss=0.0874
Epoch=179, loss=0.0957
Epoch=180, loss=0.0890
Epoch=181, loss=0.0926
Epoch=182, loss=0.0925
Epoch=183, loss=0.0980
Epoch=184, loss=0.0907
Epoch=185, loss=0.0898
Epoch=186, loss=0.0872
Epoch=187, loss=0.0961
Epoch=188, loss=0.0911
Epoch=189, loss=0.0839
Epoch=190, loss=0.0930
Epoch=191, loss=0.0844
Epoch=192, loss=0.0904
Epoch=193, loss=0.0819
Epoch=194, loss=0.0890
Epoch=195, loss=0.0767
Epoch=196, loss=0.0810
Epoch=197, loss=0.0786
Epoch=198, loss=0.0844
Epoch=199, loss=0.0807
Epoch=200, loss=0.0752
Epoch=201, loss=0.0773
Epoch=202, loss=0.0785
Epoch=203, loss=0.0750
Epoch=204, loss=0.0761
Epoch=205, loss=0.0741
Epoch=206, loss=0.0755
Epoch=207, loss=0.0817
Epoch=208, loss=0.0792
Epoch=209, loss=0.0739
Epoch=210, loss=0.0735
Epoch=211, loss=0.0800
Epoch=212, loss=0.0783
Epoch=213, loss=0.0730
Epoch=214, loss=0.0700
Epoch=215, loss=0.0804
Epoch=216, loss=0.0681
Epoch=217, loss=0.0703
Epoch=218, loss=0.0722
Epoch=219, loss=0.0655
Epoch=220, loss=0.0666
Epoch=221, loss=0.0772
Epoch=222, loss=0.0750
Epoch=223, loss=0.0785
Epoch=224, loss=0.0677
Epoch=225, loss=0.0667
Epoch=226, loss=0.0674
Epoch=227, loss=0.0570
Epoch=228, loss=0.0675
Epoch=229, loss=0.0605
Epoch=230, loss=0.0655
Epoch=231, loss=0.0711
Epoch=232, loss=0.0626
Epoch=233, loss=0.0705
Epoch=234, loss=0.0644
Epoch=235, loss=0.0601
Epoch=236, loss=0.0688
Epoch=237, loss=0.0726
Epoch=238, loss=0.0633
Epoch=239, loss=0.0630
Epoch=240, loss=0.0556
Epoch=241, loss=0.0634
Epoch=242, loss=0.0621
Epoch=243, loss=0.0614
Epoch=244, loss=0.0600
Epoch=245, loss=0.0602
Epoch=246, loss=0.0646
Epoch=247, loss=0.0647
Epoch=248, loss=0.0561
Epoch=249, loss=0.0536
Epoch=250, loss=0.0572
Epoch=251, loss=0.0609
Epoch=252, loss=0.0611
Epoch=253, loss=0.0662
Epoch=254, loss=0.0535
Epoch=255, loss=0.0522
Epoch=256, loss=0.0570
Epoch=257, loss=0.0472
Epoch=258, loss=0.0508
Epoch=259, loss=0.0524
Epoch=260, loss=0.0526
Epoch=261, loss=0.0631
Epoch=262, loss=0.0584
Epoch=263, loss=0.0585
Epoch=264, loss=0.0540
Epoch=265, loss=0.0550
Epoch=266, loss=0.0597
Epoch=267, loss=0.0506
Epoch=268, loss=0.0564
Epoch=269, loss=0.0541
Epoch=270, loss=0.0503
Epoch=271, loss=0.0584
Epoch=272, loss=0.0494
Epoch=273, loss=0.0579
Epoch=274, loss=0.0553
Epoch=275, loss=0.0463
Epoch=276, loss=0.0459
Epoch=277, loss=0.0501
Epoch=278, loss=0.0541
Epoch=279, loss=0.0513
Epoch=280, loss=0.0501
Epoch=281, loss=0.0656
Epoch=282, loss=0.0504
Epoch=283, loss=0.0415
Epoch=284, loss=0.0541
Epoch=285, loss=0.0548
Epoch=286, loss=0.0483
Epoch=287, loss=0.0458
Epoch=288, loss=0.0543
Epoch=289, loss=0.0457
Epoch=290, loss=0.0589
Epoch=291, loss=0.0417
Epoch=292, loss=0.0447
Epoch=293, loss=0.0439
Epoch=294, loss=0.0492
Epoch=295, loss=0.0426
Epoch=296, loss=0.0495
Epoch=297, loss=0.0481
Epoch=298, loss=0.0436
Epoch=299, loss=0.0505
Epoch=300, loss=0.0460
Epoch=301, loss=0.0481
Epoch=302, loss=0.0442
Epoch=303, loss=0.0383
Epoch=304, loss=0.0444
Epoch=305, loss=0.0536
Epoch=306, loss=0.0454
Epoch=307, loss=0.0518
Epoch=308, loss=0.0443
Epoch=309, loss=0.0419
Epoch=310, loss=0.0418
Epoch=311, loss=0.0371
Epoch=312, loss=0.0422
Epoch=313, loss=0.0421
Epoch=314, loss=0.0410
Epoch=315, loss=0.0518
Epoch=316, loss=0.0471
Epoch=317, loss=0.0432
Epoch=318, loss=0.0400
Epoch=319, loss=0.0452
Epoch=320, loss=0.0376
Epoch=321, loss=0.0303
Epoch=322, loss=0.0424
Epoch=323, loss=0.0403
Epoch=324, loss=0.0497
Epoch=325, loss=0.0437
Epoch=326, loss=0.0433
Epoch=327, loss=0.0537
Epoch=328, loss=0.0379
Epoch=329, loss=0.0405
Epoch=330, loss=0.0418
Epoch=331, loss=0.0350
Epoch=332, loss=0.0362
Epoch=333, loss=0.0443
Epoch=334, loss=0.0433
Epoch=335, loss=0.0459
Epoch=336, loss=0.0458
Epoch=337, loss=0.0330
Epoch=338, loss=0.0319
Epoch=339, loss=0.0410
Epoch=340, loss=0.0334
Epoch=341, loss=0.0469
Early stopping!
Loading 321th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7710+-0.0081, F1Ma=0.7386+-0.0127, acc=0.7710+-0.0081
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.765483307040204, 0.7749861751976163, 0.7759146506126476, 0.780777803196709, 0.8142856955528259, 0.7160000205039978, 0.6934235692024231, 0.8142856955528259, 0.7120000123977661, 0.6832688450813293, 0.7036921881072677, 0.026900291239213944, 0.6376950336999908, 0.04124841151711339, 0.7036921881072677, 0.026900291239213912], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7811664190266936, 0.7880120842719105, 0.7700150146365702, 0.7799402072446575, 0.8999999761581421, 0.7760000228881836, 0.759671151638031, 0.9071428775787354, 0.7739999890327454, 0.7606382966041565, 0.771006607073455, 0.008116767964061656, 0.7385701641692017, 0.012703988167904221, 0.771006607073455, 0.008116767964061628]]
=== train DGI model ===
Epoch=000, loss=0.6931
Epoch=001, loss=0.6930
Epoch=002, loss=0.6928
Epoch=003, loss=0.6926
Epoch=004, loss=0.6923
Epoch=005, loss=0.6920
Epoch=006, loss=0.6916
Epoch=007, loss=0.6911
Epoch=008, loss=0.6906
Epoch=009, loss=0.6900
Epoch=010, loss=0.6892
Epoch=011, loss=0.6884
Epoch=012, loss=0.6873
Epoch=013, loss=0.6862
Epoch=014, loss=0.6850
Epoch=015, loss=0.6835
Epoch=016, loss=0.6818
Epoch=017, loss=0.6801
Epoch=018, loss=0.6778
Epoch=019, loss=0.6758
Epoch=020, loss=0.6731
Epoch=021, loss=0.6701
Epoch=022, loss=0.6672
Epoch=023, loss=0.6638
Epoch=024, loss=0.6598
Epoch=025, loss=0.6557
Epoch=026, loss=0.6517
Epoch=027, loss=0.6466
Epoch=028, loss=0.6414
Epoch=029, loss=0.6356
Epoch=030, loss=0.6294
Epoch=031, loss=0.6226
Epoch=032, loss=0.6157
Epoch=033, loss=0.6069
Epoch=034, loss=0.6012
Epoch=035, loss=0.5932
Epoch=036, loss=0.5832
Epoch=037, loss=0.5737
Epoch=038, loss=0.5650
Epoch=039, loss=0.5530
Epoch=040, loss=0.5443
Epoch=041, loss=0.5307
Epoch=042, loss=0.5192
Epoch=043, loss=0.5071
Epoch=044, loss=0.4997
Epoch=045, loss=0.4817
Epoch=046, loss=0.4706
Epoch=047, loss=0.4555
Epoch=048, loss=0.4446
Epoch=049, loss=0.4337
Epoch=050, loss=0.4205
Epoch=051, loss=0.4062
Epoch=052, loss=0.3909
Epoch=053, loss=0.3745
Epoch=054, loss=0.3644
Epoch=055, loss=0.3526
Epoch=056, loss=0.3336
Epoch=057, loss=0.3214
Epoch=058, loss=0.3173
Epoch=059, loss=0.2983
Epoch=060, loss=0.2898
Epoch=061, loss=0.2791
Epoch=062, loss=0.2654
Epoch=063, loss=0.2675
Epoch=064, loss=0.2460
Epoch=065, loss=0.2469
Epoch=066, loss=0.2313
Epoch=067, loss=0.2296
Epoch=068, loss=0.2128
Epoch=069, loss=0.2109
Epoch=070, loss=0.1931
Epoch=071, loss=0.1906
Epoch=072, loss=0.1806
Epoch=073, loss=0.1787
Epoch=074, loss=0.1758
Epoch=075, loss=0.1724
Epoch=076, loss=0.1566
Epoch=077, loss=0.1576
Epoch=078, loss=0.1501
Epoch=079, loss=0.1497
Epoch=080, loss=0.1403
Epoch=081, loss=0.1394
Epoch=082, loss=0.1465
Epoch=083, loss=0.1434
Epoch=084, loss=0.1325
Epoch=085, loss=0.1227
Epoch=086, loss=0.1209
Epoch=087, loss=0.1173
Epoch=088, loss=0.1183
Epoch=089, loss=0.1172
Epoch=090, loss=0.1089
Epoch=091, loss=0.1062
Epoch=092, loss=0.1144
Epoch=093, loss=0.1065
Epoch=094, loss=0.1074
Epoch=095, loss=0.0997
Epoch=096, loss=0.1036
Epoch=097, loss=0.0955
Epoch=098, loss=0.1013
Epoch=099, loss=0.1015
Epoch=100, loss=0.0967
Epoch=101, loss=0.0879
Epoch=102, loss=0.0940
Epoch=103, loss=0.0919
Epoch=104, loss=0.0913
Epoch=105, loss=0.0966
Epoch=106, loss=0.0883
Epoch=107, loss=0.0906
Epoch=108, loss=0.0844
Epoch=109, loss=0.0852
Epoch=110, loss=0.0797
Epoch=111, loss=0.0786
Epoch=112, loss=0.0774
Epoch=113, loss=0.0863
Epoch=114, loss=0.0805
Epoch=115, loss=0.0823
Epoch=116, loss=0.0825
Epoch=117, loss=0.0749
Epoch=118, loss=0.0842
Epoch=119, loss=0.0756
Epoch=120, loss=0.0751
Epoch=121, loss=0.0669
Epoch=122, loss=0.0789
Epoch=123, loss=0.0698
Epoch=124, loss=0.0720
Epoch=125, loss=0.0703
Epoch=126, loss=0.0702
Epoch=127, loss=0.0663
Epoch=128, loss=0.0651
Epoch=129, loss=0.0672
Epoch=130, loss=0.0818
Epoch=131, loss=0.0713
Epoch=132, loss=0.0668
Epoch=133, loss=0.0577
Epoch=134, loss=0.0580
Epoch=135, loss=0.0599
Epoch=136, loss=0.0684
Epoch=137, loss=0.0665
Epoch=138, loss=0.0668
Epoch=139, loss=0.0598
Epoch=140, loss=0.0533
Epoch=141, loss=0.0575
Epoch=142, loss=0.0620
Epoch=143, loss=0.0667
Epoch=144, loss=0.0589
Epoch=145, loss=0.0530
Epoch=146, loss=0.0594
Epoch=147, loss=0.0656
Epoch=148, loss=0.0532
Epoch=149, loss=0.0600
Epoch=150, loss=0.0597
Epoch=151, loss=0.0552
Epoch=152, loss=0.0631
Epoch=153, loss=0.0525
Epoch=154, loss=0.0528
Epoch=155, loss=0.0518
Epoch=156, loss=0.0598
Epoch=157, loss=0.0532
Epoch=158, loss=0.0557
Epoch=159, loss=0.0551
Epoch=160, loss=0.0635
Epoch=161, loss=0.0551
Epoch=162, loss=0.0474
Epoch=163, loss=0.0527
Epoch=164, loss=0.0616
Epoch=165, loss=0.0421
Epoch=166, loss=0.0531
Epoch=167, loss=0.0479
Epoch=168, loss=0.0574
Epoch=169, loss=0.0524
Epoch=170, loss=0.0519
Epoch=171, loss=0.0544
Epoch=172, loss=0.0470
Epoch=173, loss=0.0530
Epoch=174, loss=0.0622
Epoch=175, loss=0.0398
Epoch=176, loss=0.0477
Epoch=177, loss=0.0508
Epoch=178, loss=0.0482
Epoch=179, loss=0.0408
Epoch=180, loss=0.0536
Epoch=181, loss=0.0386
Epoch=182, loss=0.0369
Epoch=183, loss=0.0442
Epoch=184, loss=0.0428
Epoch=185, loss=0.0432
Epoch=186, loss=0.0485
Epoch=187, loss=0.0494
Epoch=188, loss=0.0416
Epoch=189, loss=0.0415
Epoch=190, loss=0.0456
Epoch=191, loss=0.0424
Epoch=192, loss=0.0438
Epoch=193, loss=0.0400
Epoch=194, loss=0.0549
Epoch=195, loss=0.0475
Epoch=196, loss=0.0438
Epoch=197, loss=0.0479
Epoch=198, loss=0.0455
Epoch=199, loss=0.0404
Epoch=200, loss=0.0458
Epoch=201, loss=0.0449
Epoch=202, loss=0.0439
Early stopping!
Loading 182th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7936+-0.0103, F1Ma=0.7581+-0.0237, acc=0.7936+-0.0103
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.765483307040204, 0.7749861751976163, 0.7759146506126476, 0.780777803196709, 0.8142856955528259, 0.7160000205039978, 0.6934235692024231, 0.8142856955528259, 0.7120000123977661, 0.6832688450813293, 0.7036921881072677, 0.026900291239213944, 0.6376950336999908, 0.04124841151711339, 0.7036921881072677, 0.026900291239213912], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7811664190266936, 0.7880120842719105, 0.7700150146365702, 0.7799402072446575, 0.8999999761581421, 0.7760000228881836, 0.759671151638031, 0.9071428775787354, 0.7739999890327454, 0.7606382966041565, 0.771006607073455, 0.008116767964061656, 0.7385701641692017, 0.012703988167904221, 0.771006607073455, 0.008116767964061628], ['DGI', 'heat', 64, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7609552637494796, 0.768795518739591, 0.7527949908003846, 0.7621435789708171, 0.8999999761581421, 0.8159999847412109, 0.7799806594848633, 0.9071428775787354, 0.8159999847412109, 0.7799806594848633, 0.7936261173727168, 0.010256273535978683, 0.7580656937172139, 0.023733598467359413, 0.7936261173727168, 0.010256273535978697]]
=== train DGI model ===
Epoch=000, loss=0.6932
Epoch=001, loss=0.6927
Epoch=002, loss=0.6920
Epoch=003, loss=0.6913
Epoch=004, loss=0.6902
Epoch=005, loss=0.6888
Epoch=006, loss=0.6871
Epoch=007, loss=0.6848
Epoch=008, loss=0.6823
Epoch=009, loss=0.6792
Epoch=010, loss=0.6762
Epoch=011, loss=0.6709
Epoch=012, loss=0.6662
Epoch=013, loss=0.6602
Epoch=014, loss=0.6547
Epoch=015, loss=0.6473
Epoch=016, loss=0.6378
Epoch=017, loss=0.6280
Epoch=018, loss=0.6180
Epoch=019, loss=0.6054
Epoch=020, loss=0.5944
Epoch=021, loss=0.5787
Epoch=022, loss=0.5652
Epoch=023, loss=0.5488
Epoch=024, loss=0.5347
Epoch=025, loss=0.5131
Epoch=026, loss=0.4985
Epoch=027, loss=0.4776
Epoch=028, loss=0.4586
Epoch=029, loss=0.4309
Epoch=030, loss=0.4214
Epoch=031, loss=0.4047
Epoch=032, loss=0.3758
Epoch=033, loss=0.3555
Epoch=034, loss=0.3429
Epoch=035, loss=0.3246
Epoch=036, loss=0.3094
Epoch=037, loss=0.2918
Epoch=038, loss=0.2679
Epoch=039, loss=0.2576
Epoch=040, loss=0.2466
Epoch=041, loss=0.2166
Epoch=042, loss=0.2211
Epoch=043, loss=0.2029
Epoch=044, loss=0.1864
Epoch=045, loss=0.1828
Epoch=046, loss=0.1979
Epoch=047, loss=0.1752
Epoch=048, loss=0.1520
Epoch=049, loss=0.1590
Epoch=050, loss=0.1521
Epoch=051, loss=0.1422
Epoch=052, loss=0.1361
Epoch=053, loss=0.1414
Epoch=054, loss=0.1234
Epoch=055, loss=0.1355
Epoch=056, loss=0.1180
Epoch=057, loss=0.1103
Epoch=058, loss=0.1203
Epoch=059, loss=0.1091
Epoch=060, loss=0.1059
Epoch=061, loss=0.1136
Epoch=062, loss=0.1057
Epoch=063, loss=0.0956
Epoch=064, loss=0.0908
Epoch=065, loss=0.1027
Epoch=066, loss=0.0965
Epoch=067, loss=0.0954
Epoch=068, loss=0.0998
Epoch=069, loss=0.0853
Epoch=070, loss=0.0775
Epoch=071, loss=0.0987
Epoch=072, loss=0.0887
Epoch=073, loss=0.0968
Epoch=074, loss=0.0768
Epoch=075, loss=0.0829
Epoch=076, loss=0.0769
Epoch=077, loss=0.0772
Epoch=078, loss=0.0863
Epoch=079, loss=0.0826
Epoch=080, loss=0.0740
Epoch=081, loss=0.0798
Epoch=082, loss=0.0743
Epoch=083, loss=0.0723
Epoch=084, loss=0.0707
Epoch=085, loss=0.0774
Epoch=086, loss=0.0659
Epoch=087, loss=0.0737
Epoch=088, loss=0.0850
Epoch=089, loss=0.0793
Epoch=090, loss=0.0681
Epoch=091, loss=0.0578
Epoch=092, loss=0.0726
Epoch=093, loss=0.0750
Epoch=094, loss=0.0718
Epoch=095, loss=0.0618
Epoch=096, loss=0.0604
Epoch=097, loss=0.0674
Epoch=098, loss=0.0648
Epoch=099, loss=0.0664
Epoch=100, loss=0.0606
Epoch=101, loss=0.0732
Epoch=102, loss=0.0635
Epoch=103, loss=0.0674
Epoch=104, loss=0.0616
Epoch=105, loss=0.0557
Epoch=106, loss=0.0542
Epoch=107, loss=0.0645
Epoch=108, loss=0.0602
Epoch=109, loss=0.0629
Epoch=110, loss=0.0546
Epoch=111, loss=0.0610
Epoch=112, loss=0.0568
Epoch=113, loss=0.0593
Epoch=114, loss=0.0548
Epoch=115, loss=0.0593
Epoch=116, loss=0.0628
Epoch=117, loss=0.0496
Epoch=118, loss=0.0560
Epoch=119, loss=0.0559
Epoch=120, loss=0.0563
Epoch=121, loss=0.0539
Epoch=122, loss=0.0592
Epoch=123, loss=0.0540
Epoch=124, loss=0.0499
Epoch=125, loss=0.0644
Epoch=126, loss=0.0558
Epoch=127, loss=0.0427
Epoch=128, loss=0.0481
Epoch=129, loss=0.0518
Epoch=130, loss=0.0526
Epoch=131, loss=0.0531
Epoch=132, loss=0.0518
Epoch=133, loss=0.0480
Epoch=134, loss=0.0522
Epoch=135, loss=0.0539
Epoch=136, loss=0.0406
Epoch=137, loss=0.0466
Epoch=138, loss=0.0472
Epoch=139, loss=0.0458
Epoch=140, loss=0.0445
Epoch=141, loss=0.0448
Epoch=142, loss=0.0537
Epoch=143, loss=0.0476
Epoch=144, loss=0.0477
Epoch=145, loss=0.0385
Epoch=146, loss=0.0424
Epoch=147, loss=0.0500
Epoch=148, loss=0.0488
Epoch=149, loss=0.0437
Epoch=150, loss=0.0531
Epoch=151, loss=0.0413
Epoch=152, loss=0.0510
Epoch=153, loss=0.0429
Epoch=154, loss=0.0535
Epoch=155, loss=0.0455
Epoch=156, loss=0.0622
Epoch=157, loss=0.0401
Epoch=158, loss=0.0430
Epoch=159, loss=0.0437
Epoch=160, loss=0.0411
Epoch=161, loss=0.0425
Epoch=162, loss=0.0469
Epoch=163, loss=0.0507
Epoch=164, loss=0.0465
Epoch=165, loss=0.0497
Early stopping!
Loading 145th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7987+-0.0146, F1Ma=0.7697+-0.0314, acc=0.7987+-0.0146
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.765483307040204, 0.7749861751976163, 0.7759146506126476, 0.780777803196709, 0.8142856955528259, 0.7160000205039978, 0.6934235692024231, 0.8142856955528259, 0.7120000123977661, 0.6832688450813293, 0.7036921881072677, 0.026900291239213944, 0.6376950336999908, 0.04124841151711339, 0.7036921881072677, 0.026900291239213912], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7811664190266936, 0.7880120842719105, 0.7700150146365702, 0.7799402072446575, 0.8999999761581421, 0.7760000228881836, 0.759671151638031, 0.9071428775787354, 0.7739999890327454, 0.7606382966041565, 0.771006607073455, 0.008116767964061656, 0.7385701641692017, 0.012703988167904221, 0.771006607073455, 0.008116767964061628], ['DGI', 'heat', 64, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7609552637494796, 0.768795518739591, 0.7527949908003846, 0.7621435789708171, 0.8999999761581421, 0.8159999847412109, 0.7799806594848633, 0.9071428775787354, 0.8159999847412109, 0.7799806594848633, 0.7936261173727168, 0.010256273535978683, 0.7580656937172139, 0.023733598467359413, 0.7936261173727168, 0.010256273535978697], ['DGI', 'heat', 128, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7823021968267132, 0.7749094685584634, 0.782701842443533, 0.7743268097139444, 0.9428571462631226, 0.8199999928474426, 0.7964216470718384, 0.9428571462631226, 0.8199999928474426, 0.7959381341934204, 0.7986785853089777, 0.014618665854946747, 0.7696576246135087, 0.03138471857782909, 0.798678585308978, 0.014618665854946743]]
=== train DGI model ===
Epoch=000, loss=0.6931
Epoch=001, loss=0.6917
Epoch=002, loss=0.6892
Epoch=003, loss=0.6859
Epoch=004, loss=0.6808
Epoch=005, loss=0.6746
Epoch=006, loss=0.6673
Epoch=007, loss=0.6575
Epoch=008, loss=0.6462
Epoch=009, loss=0.6320
Epoch=010, loss=0.6173
Epoch=011, loss=0.6001
Epoch=012, loss=0.5788
Epoch=013, loss=0.5596
Epoch=014, loss=0.5352
Epoch=015, loss=0.5122
Epoch=016, loss=0.4904
Epoch=017, loss=0.4530
Epoch=018, loss=0.4238
Epoch=019, loss=0.4136
Epoch=020, loss=0.3782
Epoch=021, loss=0.3438
Epoch=022, loss=0.3289
Epoch=023, loss=0.3144
Epoch=024, loss=0.2685
Epoch=025, loss=0.2841
Epoch=026, loss=0.2418
Epoch=027, loss=0.2232
Epoch=028, loss=0.2061
Epoch=029, loss=0.1955
Epoch=030, loss=0.1855
Epoch=031, loss=0.1785
Epoch=032, loss=0.1696
Epoch=033, loss=0.1537
Epoch=034, loss=0.1495
Epoch=035, loss=0.1365
Epoch=036, loss=0.1347
Epoch=037, loss=0.1347
Epoch=038, loss=0.1198
Epoch=039, loss=0.1281
Epoch=040, loss=0.1226
Epoch=041, loss=0.1213
Epoch=042, loss=0.1082
Epoch=043, loss=0.1014
Epoch=044, loss=0.1049
Epoch=045, loss=0.0978
Epoch=046, loss=0.0978
Epoch=047, loss=0.0931
Epoch=048, loss=0.1001
Epoch=049, loss=0.0984
Epoch=050, loss=0.1108
Epoch=051, loss=0.1001
Epoch=052, loss=0.0764
Epoch=053, loss=0.0965
Epoch=054, loss=0.0890
Epoch=055, loss=0.0842
Epoch=056, loss=0.0856
Epoch=057, loss=0.0874
Epoch=058, loss=0.0913
Epoch=059, loss=0.0799
Epoch=060, loss=0.0813
Epoch=061, loss=0.0791
Epoch=062, loss=0.0717
Epoch=063, loss=0.0719
Epoch=064, loss=0.0850
Epoch=065, loss=0.0632
Epoch=066, loss=0.0698
Epoch=067, loss=0.0693
Epoch=068, loss=0.0674
Epoch=069, loss=0.0751
Epoch=070, loss=0.0606
Epoch=071, loss=0.0521
Epoch=072, loss=0.0621
Epoch=073, loss=0.0598
Epoch=074, loss=0.0598
Epoch=075, loss=0.0558
Epoch=076, loss=0.0660
Epoch=077, loss=0.0648
Epoch=078, loss=0.0528
Epoch=079, loss=0.0478
Epoch=080, loss=0.0551
Epoch=081, loss=0.0523
Epoch=082, loss=0.0696
Epoch=083, loss=0.0660
Epoch=084, loss=0.0603
Epoch=085, loss=0.0525
Epoch=086, loss=0.0504
Epoch=087, loss=0.0642
Epoch=088, loss=0.0521
Epoch=089, loss=0.0480
Epoch=090, loss=0.0599
Epoch=091, loss=0.0586
Epoch=092, loss=0.0523
Epoch=093, loss=0.0409
Epoch=094, loss=0.0459
Epoch=095, loss=0.0560
Epoch=096, loss=0.0556
Epoch=097, loss=0.0434
Epoch=098, loss=0.0421
Epoch=099, loss=0.0404
Epoch=100, loss=0.0415
Epoch=101, loss=0.0336
Epoch=102, loss=0.0490
Epoch=103, loss=0.0373
Epoch=104, loss=0.0365
Epoch=105, loss=0.0497
Epoch=106, loss=0.0536
Epoch=107, loss=0.0491
Epoch=108, loss=0.0442
Epoch=109, loss=0.0370
Epoch=110, loss=0.0475
Epoch=111, loss=0.0434
Epoch=112, loss=0.0417
Epoch=113, loss=0.0555
Epoch=114, loss=0.0399
Epoch=115, loss=0.0412
Epoch=116, loss=0.0462
Epoch=117, loss=0.0516
Epoch=118, loss=0.0418
Epoch=119, loss=0.0422
Epoch=120, loss=0.0414
Epoch=121, loss=0.0406
Early stopping!
Loading 101th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.8078+-0.0104, F1Ma=0.7839+-0.0150, acc=0.8078+-0.0104
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.765483307040204, 0.7749861751976163, 0.7759146506126476, 0.780777803196709, 0.8142856955528259, 0.7160000205039978, 0.6934235692024231, 0.8142856955528259, 0.7120000123977661, 0.6832688450813293, 0.7036921881072677, 0.026900291239213944, 0.6376950336999908, 0.04124841151711339, 0.7036921881072677, 0.026900291239213912], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7811664190266936, 0.7880120842719105, 0.7700150146365702, 0.7799402072446575, 0.8999999761581421, 0.7760000228881836, 0.759671151638031, 0.9071428775787354, 0.7739999890327454, 0.7606382966041565, 0.771006607073455, 0.008116767964061656, 0.7385701641692017, 0.012703988167904221, 0.771006607073455, 0.008116767964061628], ['DGI', 'heat', 64, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7609552637494796, 0.768795518739591, 0.7527949908003846, 0.7621435789708171, 0.8999999761581421, 0.8159999847412109, 0.7799806594848633, 0.9071428775787354, 0.8159999847412109, 0.7799806594848633, 0.7936261173727168, 0.010256273535978683, 0.7580656937172139, 0.023733598467359413, 0.7936261173727168, 0.010256273535978697], ['DGI', 'heat', 128, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7823021968267132, 0.7749094685584634, 0.782701842443533, 0.7743268097139444, 0.9428571462631226, 0.8199999928474426, 0.7964216470718384, 0.9428571462631226, 0.8199999928474426, 0.7959381341934204, 0.7986785853089777, 0.014618665854946747, 0.7696576246135087, 0.03138471857782909, 0.798678585308978, 0.014618665854946743], ['DGI', 'heat', 256, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.76693665282107, 0.7573274713971213, 0.7610440393333069, 0.7547804322497277, 0.9214285612106323, 0.8059999942779541, 0.7756286263465881, 0.9142857193946838, 0.8040000200271606, 0.7756286263465881, 0.8077730275942478, 0.01037225998424942, 0.7838841161467573, 0.014976523932594205, 0.8077730275942478, 0.01037225998424942]]
=== train DGI model ===
Epoch=000, loss=0.6932
Epoch=001, loss=0.6913
Epoch=002, loss=0.6874
Epoch=003, loss=0.6764
Epoch=004, loss=0.6723
Epoch=005, loss=0.6551
Epoch=006, loss=0.6464
Epoch=007, loss=0.6265
Epoch=008, loss=0.6068
Epoch=009, loss=0.5902
Epoch=010, loss=0.5570
Epoch=011, loss=0.5259
Epoch=012, loss=0.5115
Epoch=013, loss=0.4824
Epoch=014, loss=0.4381
Epoch=015, loss=0.4111
Epoch=016, loss=0.4010
Epoch=017, loss=0.3626
Epoch=018, loss=0.3165
Epoch=019, loss=0.3212
Epoch=020, loss=0.2840
Epoch=021, loss=0.2440
Epoch=022, loss=0.2413
Epoch=023, loss=0.2281
Epoch=024, loss=0.1809
Epoch=025, loss=0.1951
Epoch=026, loss=0.1673
Epoch=027, loss=0.1354
Epoch=028, loss=0.1424
Epoch=029, loss=0.1344
Epoch=030, loss=0.1036
Epoch=031, loss=0.1167
Epoch=032, loss=0.1098
Epoch=033, loss=0.1002
Epoch=034, loss=0.0880
Epoch=035, loss=0.0827
Epoch=036, loss=0.0914
Epoch=037, loss=0.0674
Epoch=038, loss=0.0752
Epoch=039, loss=0.0611
Epoch=040, loss=0.0631
Epoch=041, loss=0.0681
Epoch=042, loss=0.0550
Epoch=043, loss=0.0604
Epoch=044, loss=0.0517
Epoch=045, loss=0.0466
Epoch=046, loss=0.0444
Epoch=047, loss=0.0492
Epoch=048, loss=0.0466
Epoch=049, loss=0.0415
Epoch=050, loss=0.0340
Epoch=051, loss=0.0371
Epoch=052, loss=0.0444
Epoch=053, loss=0.0358
Epoch=054, loss=0.0325
Epoch=055, loss=0.0269
Epoch=056, loss=0.0255
Epoch=057, loss=0.0420
Epoch=058, loss=0.0261
Epoch=059, loss=0.0258
Epoch=060, loss=0.0244
Epoch=061, loss=0.0322
Epoch=062, loss=0.0300
Epoch=063, loss=0.0331
Epoch=064, loss=0.0263
Epoch=065, loss=0.0245
Epoch=066, loss=0.0185
Epoch=067, loss=0.0270
Epoch=068, loss=0.0210
Epoch=069, loss=0.0282
Epoch=070, loss=0.0254
Epoch=071, loss=0.0215
Epoch=072, loss=0.0179
Epoch=073, loss=0.0161
Epoch=074, loss=0.0207
Epoch=075, loss=0.0169
Epoch=076, loss=0.0150
Epoch=077, loss=0.0180
Epoch=078, loss=0.0203
Epoch=079, loss=0.0204
Epoch=080, loss=0.0174
Epoch=081, loss=0.0152
Epoch=082, loss=0.0233
Epoch=083, loss=0.0166
Epoch=084, loss=0.0150
Epoch=085, loss=0.0182
Epoch=086, loss=0.0113
Epoch=087, loss=0.0178
Epoch=088, loss=0.0130
Epoch=089, loss=0.0100
Epoch=090, loss=0.0180
Epoch=091, loss=0.0137
Epoch=092, loss=0.0120
Epoch=093, loss=0.0054
Epoch=094, loss=0.0122
Epoch=095, loss=0.0149
Epoch=096, loss=0.0077
Epoch=097, loss=0.0137
Epoch=098, loss=0.0126
Epoch=099, loss=0.0096
Epoch=100, loss=0.0121
Epoch=101, loss=0.0092
Epoch=102, loss=0.0090
Epoch=103, loss=0.0093
Epoch=104, loss=0.0139
Epoch=105, loss=0.0129
Epoch=106, loss=0.0142
Epoch=107, loss=0.0097
Epoch=108, loss=0.0081
Epoch=109, loss=0.0083
Epoch=110, loss=0.0162
Epoch=111, loss=0.0108
Epoch=112, loss=0.0085
Epoch=113, loss=0.0095
Early stopping!
Loading 93th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7960+-0.0107, F1Ma=0.7777+-0.0137, acc=0.7960+-0.0107
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.765483307040204, 0.7749861751976163, 0.7759146506126476, 0.780777803196709, 0.8142856955528259, 0.7160000205039978, 0.6934235692024231, 0.8142856955528259, 0.7120000123977661, 0.6832688450813293, 0.7036921881072677, 0.026900291239213944, 0.6376950336999908, 0.04124841151711339, 0.7036921881072677, 0.026900291239213912], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7811664190266936, 0.7880120842719105, 0.7700150146365702, 0.7799402072446575, 0.8999999761581421, 0.7760000228881836, 0.759671151638031, 0.9071428775787354, 0.7739999890327454, 0.7606382966041565, 0.771006607073455, 0.008116767964061656, 0.7385701641692017, 0.012703988167904221, 0.771006607073455, 0.008116767964061628], ['DGI', 'heat', 64, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7609552637494796, 0.768795518739591, 0.7527949908003846, 0.7621435789708171, 0.8999999761581421, 0.8159999847412109, 0.7799806594848633, 0.9071428775787354, 0.8159999847412109, 0.7799806594848633, 0.7936261173727168, 0.010256273535978683, 0.7580656937172139, 0.023733598467359413, 0.7936261173727168, 0.010256273535978697], ['DGI', 'heat', 128, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7823021968267132, 0.7749094685584634, 0.782701842443533, 0.7743268097139444, 0.9428571462631226, 0.8199999928474426, 0.7964216470718384, 0.9428571462631226, 0.8199999928474426, 0.7959381341934204, 0.7986785853089777, 0.014618665854946747, 0.7696576246135087, 0.03138471857782909, 0.798678585308978, 0.014618665854946743], ['DGI', 'heat', 256, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.76693665282107, 0.7573274713971213, 0.7610440393333069, 0.7547804322497277, 0.9214285612106323, 0.8059999942779541, 0.7756286263465881, 0.9142857193946838, 0.8040000200271606, 0.7756286263465881, 0.8077730275942478, 0.01037225998424942, 0.7838841161467573, 0.014976523932594205, 0.8077730275942478, 0.01037225998424942], ['DGI', 'heat', 512, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7507468668677972, 0.7620061291229392, 0.7389289559246605, 0.7637939395322211, 0.9357143044471741, 0.8240000009536743, 0.7838491201400757, 0.9428571462631226, 0.8240000009536743, 0.7852997779846191, 0.7960357559269335, 0.010695753757501727, 0.7776986162350115, 0.013747990411754977, 0.7960357559269335, 0.010695753757501727]]
