My SLURM_ARRAY_TASK_ID:  9
My SLURM_ARRAY_JOB_ID:  4522289
DGI
Cora
result file is 4522289_9
Found it
Dataset: Cora():
======================
Number of graphs: 1
Number of features: 1433
Number of classes: 7
Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])
==============================================================
Number of nodes: 2708
Number of edges: 10556
Average node degree: 3.90
Number of training nodes: 140
Training node label rate: 0.05
Contains isolated nodes: False
Contains self-loops: False
Is undirected: True
/scratch/midway3/cdonnat/gnumap/experiments/results/DGI_results_Cora_4522289_9.csv
=== train DGI model ===
Epoch=000, loss=0.6931
Epoch=001, loss=0.6931
Epoch=002, loss=0.6930
Epoch=003, loss=0.6930
Epoch=004, loss=0.6930
Epoch=005, loss=0.6929
Epoch=006, loss=0.6929
Epoch=007, loss=0.6928
Epoch=008, loss=0.6928
Epoch=009, loss=0.6928
Epoch=010, loss=0.6927
Epoch=011, loss=0.6927
Epoch=012, loss=0.6926
Epoch=013, loss=0.6925
Epoch=014, loss=0.6925
Epoch=015, loss=0.6924
Epoch=016, loss=0.6923
Epoch=017, loss=0.6922
Epoch=018, loss=0.6921
Epoch=019, loss=0.6920
Epoch=020, loss=0.6919
Epoch=021, loss=0.6918
Epoch=022, loss=0.6917
Epoch=023, loss=0.6916
Epoch=024, loss=0.6914
Epoch=025, loss=0.6913
Epoch=026, loss=0.6911
Epoch=027, loss=0.6910
Epoch=028, loss=0.6908
Epoch=029, loss=0.6905
Epoch=030, loss=0.6904
Epoch=031, loss=0.6902
Epoch=032, loss=0.6899
Epoch=033, loss=0.6897
Epoch=034, loss=0.6894
Epoch=035, loss=0.6892
Epoch=036, loss=0.6888
Epoch=037, loss=0.6886
Epoch=038, loss=0.6882
Epoch=039, loss=0.6878
Epoch=040, loss=0.6876
Epoch=041, loss=0.6872
Epoch=042, loss=0.6867
Epoch=043, loss=0.6863
Epoch=044, loss=0.6858
Epoch=045, loss=0.6854
Epoch=046, loss=0.6849
Epoch=047, loss=0.6845
Epoch=048, loss=0.6839
Epoch=049, loss=0.6834
Epoch=050, loss=0.6826
Epoch=051, loss=0.6823
Epoch=052, loss=0.6818
Epoch=053, loss=0.6806
Epoch=054, loss=0.6804
Epoch=055, loss=0.6795
Epoch=056, loss=0.6787
Epoch=057, loss=0.6778
Epoch=058, loss=0.6769
Epoch=059, loss=0.6765
Epoch=060, loss=0.6752
Epoch=061, loss=0.6745
Epoch=062, loss=0.6735
Epoch=063, loss=0.6726
Epoch=064, loss=0.6716
Epoch=065, loss=0.6704
Epoch=066, loss=0.6694
Epoch=067, loss=0.6682
Epoch=068, loss=0.6668
Epoch=069, loss=0.6659
Epoch=070, loss=0.6648
Epoch=071, loss=0.6632
Epoch=072, loss=0.6622
Epoch=073, loss=0.6611
Epoch=074, loss=0.6592
Epoch=075, loss=0.6574
Epoch=076, loss=0.6559
Epoch=077, loss=0.6544
Epoch=078, loss=0.6528
Epoch=079, loss=0.6507
Epoch=080, loss=0.6498
Epoch=081, loss=0.6476
Epoch=082, loss=0.6458
Epoch=083, loss=0.6448
Epoch=084, loss=0.6419
Epoch=085, loss=0.6395
Epoch=086, loss=0.6376
Epoch=087, loss=0.6363
Epoch=088, loss=0.6344
Epoch=089, loss=0.6326
Epoch=090, loss=0.6302
Epoch=091, loss=0.6274
Epoch=092, loss=0.6249
Epoch=093, loss=0.6230
Epoch=094, loss=0.6203
Epoch=095, loss=0.6181
Epoch=096, loss=0.6147
Epoch=097, loss=0.6136
Epoch=098, loss=0.6108
Epoch=099, loss=0.6076
Epoch=100, loss=0.6067
Epoch=101, loss=0.6033
Epoch=102, loss=0.6009
Epoch=103, loss=0.5985
Epoch=104, loss=0.5945
Epoch=105, loss=0.5922
Epoch=106, loss=0.5892
Epoch=107, loss=0.5857
Epoch=108, loss=0.5821
Epoch=109, loss=0.5776
Epoch=110, loss=0.5766
Epoch=111, loss=0.5731
Epoch=112, loss=0.5709
Epoch=113, loss=0.5676
Epoch=114, loss=0.5651
Epoch=115, loss=0.5604
Epoch=116, loss=0.5579
Epoch=117, loss=0.5543
Epoch=118, loss=0.5484
Epoch=119, loss=0.5494
Epoch=120, loss=0.5444
Epoch=121, loss=0.5410
Epoch=122, loss=0.5364
Epoch=123, loss=0.5348
Epoch=124, loss=0.5307
Epoch=125, loss=0.5268
Epoch=126, loss=0.5216
Epoch=127, loss=0.5196
Epoch=128, loss=0.5182
Epoch=129, loss=0.5108
Epoch=130, loss=0.5094
Epoch=131, loss=0.5051
Epoch=132, loss=0.5007
Epoch=133, loss=0.4986
Epoch=134, loss=0.4947
Epoch=135, loss=0.4891
Epoch=136, loss=0.4867
Epoch=137, loss=0.4825
Epoch=138, loss=0.4764
Epoch=139, loss=0.4781
Epoch=140, loss=0.4713
Epoch=141, loss=0.4626
Epoch=142, loss=0.4626
Epoch=143, loss=0.4580
Epoch=144, loss=0.4573
Epoch=145, loss=0.4486
Epoch=146, loss=0.4499
Epoch=147, loss=0.4427
Epoch=148, loss=0.4411
Epoch=149, loss=0.4365
Epoch=150, loss=0.4303
Epoch=151, loss=0.4271
Epoch=152, loss=0.4284
Epoch=153, loss=0.4245
Epoch=154, loss=0.4167
Epoch=155, loss=0.4140
Epoch=156, loss=0.4105
Epoch=157, loss=0.4107
Epoch=158, loss=0.4101
Epoch=159, loss=0.4012
Epoch=160, loss=0.3916
Epoch=161, loss=0.3942
Epoch=162, loss=0.3891
Epoch=163, loss=0.3904
Epoch=164, loss=0.3792
Epoch=165, loss=0.3780
Epoch=166, loss=0.3760
Epoch=167, loss=0.3731
Epoch=168, loss=0.3742
Epoch=169, loss=0.3675
Epoch=170, loss=0.3659
Epoch=171, loss=0.3590
Epoch=172, loss=0.3586
Epoch=173, loss=0.3562
Epoch=174, loss=0.3493
Epoch=175, loss=0.3472
Epoch=176, loss=0.3345
Epoch=177, loss=0.3375
Epoch=178, loss=0.3333
Epoch=179, loss=0.3356
Epoch=180, loss=0.3287
Epoch=181, loss=0.3283
Epoch=182, loss=0.3224
Epoch=183, loss=0.3200
Epoch=184, loss=0.3167
Epoch=185, loss=0.3249
Epoch=186, loss=0.3117
Epoch=187, loss=0.3088
Epoch=188, loss=0.3111
Epoch=189, loss=0.3112
Epoch=190, loss=0.2951
Epoch=191, loss=0.2934
Epoch=192, loss=0.2985
Epoch=193, loss=0.2890
Epoch=194, loss=0.2917
Epoch=195, loss=0.2853
Epoch=196, loss=0.2811
Epoch=197, loss=0.2821
Epoch=198, loss=0.2743
Epoch=199, loss=0.2753
Epoch=200, loss=0.2682
Epoch=201, loss=0.2696
Epoch=202, loss=0.2684
Epoch=203, loss=0.2659
Epoch=204, loss=0.2661
Epoch=205, loss=0.2586
Epoch=206, loss=0.2606
Epoch=207, loss=0.2526
Epoch=208, loss=0.2547
Epoch=209, loss=0.2516
Epoch=210, loss=0.2480
Epoch=211, loss=0.2508
Epoch=212, loss=0.2411
Epoch=213, loss=0.2371
Epoch=214, loss=0.2373
Epoch=215, loss=0.2368
Epoch=216, loss=0.2383
Epoch=217, loss=0.2290
Epoch=218, loss=0.2289
Epoch=219, loss=0.2298
Epoch=220, loss=0.2281
Epoch=221, loss=0.2224
Epoch=222, loss=0.2241
Epoch=223, loss=0.2230
Epoch=224, loss=0.2166
Epoch=225, loss=0.2079
Epoch=226, loss=0.2103
Epoch=227, loss=0.2044
Epoch=228, loss=0.2090
Epoch=229, loss=0.2010
Epoch=230, loss=0.1947
Epoch=231, loss=0.2008
Epoch=232, loss=0.1994
Epoch=233, loss=0.2001
Epoch=234, loss=0.1987
Epoch=235, loss=0.1916
Epoch=236, loss=0.1872
Epoch=237, loss=0.2000
Epoch=238, loss=0.1866
Epoch=239, loss=0.1905
Epoch=240, loss=0.1865
Epoch=241, loss=0.1945
Epoch=242, loss=0.1787
Epoch=243, loss=0.1816
Epoch=244, loss=0.1842
Epoch=245, loss=0.1867
Epoch=246, loss=0.1780
Epoch=247, loss=0.1772
Epoch=248, loss=0.1794
Epoch=249, loss=0.1742
Epoch=250, loss=0.1695
Epoch=251, loss=0.1771
Epoch=252, loss=0.1702
Epoch=253, loss=0.1610
Epoch=254, loss=0.1628
Epoch=255, loss=0.1623
Epoch=256, loss=0.1616
Epoch=257, loss=0.1566
Epoch=258, loss=0.1665
Epoch=259, loss=0.1589
Epoch=260, loss=0.1662
Epoch=261, loss=0.1560
Epoch=262, loss=0.1581
Epoch=263, loss=0.1536
Epoch=264, loss=0.1626
Epoch=265, loss=0.1487
Epoch=266, loss=0.1573
Epoch=267, loss=0.1519
Epoch=268, loss=0.1523
Epoch=269, loss=0.1561
Epoch=270, loss=0.1467
Epoch=271, loss=0.1417
Epoch=272, loss=0.1440
Epoch=273, loss=0.1515
Epoch=274, loss=0.1482
Epoch=275, loss=0.1443
Epoch=276, loss=0.1543
Epoch=277, loss=0.1475
Epoch=278, loss=0.1498
Epoch=279, loss=0.1475
Epoch=280, loss=0.1403
Epoch=281, loss=0.1339
Epoch=282, loss=0.1398
Epoch=283, loss=0.1401
Epoch=284, loss=0.1420
Epoch=285, loss=0.1355
Epoch=286, loss=0.1372
Epoch=287, loss=0.1374
Epoch=288, loss=0.1341
Epoch=289, loss=0.1344
Epoch=290, loss=0.1296
Epoch=291, loss=0.1301
Epoch=292, loss=0.1300
Epoch=293, loss=0.1319
Epoch=294, loss=0.1295
Epoch=295, loss=0.1226
Epoch=296, loss=0.1254
Epoch=297, loss=0.1289
Epoch=298, loss=0.1299
Epoch=299, loss=0.1233
Epoch=300, loss=0.1214
Epoch=301, loss=0.1288
Epoch=302, loss=0.1300
Epoch=303, loss=0.1393
Epoch=304, loss=0.1199
Epoch=305, loss=0.1297
Epoch=306, loss=0.1180
Epoch=307, loss=0.1261
Epoch=308, loss=0.1292
Epoch=309, loss=0.1159
Epoch=310, loss=0.1146
Epoch=311, loss=0.1206
Epoch=312, loss=0.1132
Epoch=313, loss=0.1157
Epoch=314, loss=0.1205
Epoch=315, loss=0.1162
Epoch=316, loss=0.1162
Epoch=317, loss=0.1238
Epoch=318, loss=0.1130
Epoch=319, loss=0.1182
Epoch=320, loss=0.1200
Epoch=321, loss=0.1161
Epoch=322, loss=0.1136
Epoch=323, loss=0.1075
Epoch=324, loss=0.1148
Epoch=325, loss=0.1108
Epoch=326, loss=0.1052
Epoch=327, loss=0.1181
Epoch=328, loss=0.1163
Epoch=329, loss=0.1083
Epoch=330, loss=0.1145
Epoch=331, loss=0.1055
Epoch=332, loss=0.1135
Epoch=333, loss=0.1015
Epoch=334, loss=0.1082
Epoch=335, loss=0.0981
Epoch=336, loss=0.1050
Epoch=337, loss=0.0983
Epoch=338, loss=0.1034
Epoch=339, loss=0.0979
Epoch=340, loss=0.1106
Epoch=341, loss=0.0973
Epoch=342, loss=0.0960
Epoch=343, loss=0.1128
Epoch=344, loss=0.0990
Epoch=345, loss=0.1201
Epoch=346, loss=0.1114
Epoch=347, loss=0.1060
Epoch=348, loss=0.1052
Epoch=349, loss=0.1079
Epoch=350, loss=0.0937
Epoch=351, loss=0.1009
Epoch=352, loss=0.1024
Epoch=353, loss=0.1019
Epoch=354, loss=0.0976
Epoch=355, loss=0.1022
Epoch=356, loss=0.1040
Epoch=357, loss=0.1010
Epoch=358, loss=0.1005
Epoch=359, loss=0.0940
Epoch=360, loss=0.0915
Epoch=361, loss=0.1062
Epoch=362, loss=0.0866
Epoch=363, loss=0.0974
Epoch=364, loss=0.0916
Epoch=365, loss=0.0959
Epoch=366, loss=0.0942
Epoch=367, loss=0.0988
Epoch=368, loss=0.0969
Epoch=369, loss=0.0888
Epoch=370, loss=0.0882
Epoch=371, loss=0.0949
Epoch=372, loss=0.0895
Epoch=373, loss=0.0950
Epoch=374, loss=0.0934
Epoch=375, loss=0.0937
Epoch=376, loss=0.0919
Epoch=377, loss=0.0944
Epoch=378, loss=0.0855
Epoch=379, loss=0.1036
Epoch=380, loss=0.0978
Epoch=381, loss=0.0975
Epoch=382, loss=0.0906
Epoch=383, loss=0.0947
Epoch=384, loss=0.0971
Epoch=385, loss=0.0854
Epoch=386, loss=0.0947
Epoch=387, loss=0.0830
Epoch=388, loss=0.0884
Epoch=389, loss=0.0819
Epoch=390, loss=0.0831
Epoch=391, loss=0.0893
Epoch=392, loss=0.0936
Epoch=393, loss=0.0807
Epoch=394, loss=0.0955
Epoch=395, loss=0.0895
Epoch=396, loss=0.0866
Epoch=397, loss=0.0911
Epoch=398, loss=0.0865
Epoch=399, loss=0.0908
Epoch=400, loss=0.0806
Epoch=401, loss=0.0814
Epoch=402, loss=0.0930
Epoch=403, loss=0.0844
Epoch=404, loss=0.0882
Epoch=405, loss=0.0859
Epoch=406, loss=0.0904
Epoch=407, loss=0.0851
Epoch=408, loss=0.0904
Epoch=409, loss=0.0898
Epoch=410, loss=0.0904
Epoch=411, loss=0.0771
Epoch=412, loss=0.0813
Epoch=413, loss=0.0880
Epoch=414, loss=0.0788
Epoch=415, loss=0.0847
Epoch=416, loss=0.0818
Epoch=417, loss=0.0777
Epoch=418, loss=0.0844
Epoch=419, loss=0.0801
Epoch=420, loss=0.0772
Epoch=421, loss=0.0797
Epoch=422, loss=0.0869
Epoch=423, loss=0.0809
Epoch=424, loss=0.0780
Epoch=425, loss=0.0821
Epoch=426, loss=0.0837
Epoch=427, loss=0.0791
Epoch=428, loss=0.0808
Epoch=429, loss=0.0755
Epoch=430, loss=0.0734
Epoch=431, loss=0.0701
Epoch=432, loss=0.0843
Epoch=433, loss=0.0803
Epoch=434, loss=0.0707
Epoch=435, loss=0.0837
Epoch=436, loss=0.0734
Epoch=437, loss=0.0660
Epoch=438, loss=0.0728
Epoch=439, loss=0.0687
Epoch=440, loss=0.0744
Epoch=441, loss=0.0670
Epoch=442, loss=0.0776
Epoch=443, loss=0.0822
Epoch=444, loss=0.0794
Epoch=445, loss=0.0739
Epoch=446, loss=0.0739
Epoch=447, loss=0.0696
Epoch=448, loss=0.0804
Epoch=449, loss=0.0786
Epoch=450, loss=0.0782
Epoch=451, loss=0.0713
Epoch=452, loss=0.0788
Epoch=453, loss=0.0814
Epoch=454, loss=0.0654
Epoch=455, loss=0.0704
Epoch=456, loss=0.0660
Epoch=457, loss=0.0712
Epoch=458, loss=0.0740
Epoch=459, loss=0.0752
Epoch=460, loss=0.0799
Epoch=461, loss=0.0721
Epoch=462, loss=0.0741
Epoch=463, loss=0.0735
Epoch=464, loss=0.0700
Epoch=465, loss=0.0667
Epoch=466, loss=0.0780
Epoch=467, loss=0.0678
Epoch=468, loss=0.0763
Epoch=469, loss=0.0797
Epoch=470, loss=0.0717
Epoch=471, loss=0.0643
Epoch=472, loss=0.0698
Epoch=473, loss=0.0658
Epoch=474, loss=0.0685
Epoch=475, loss=0.0690
Epoch=476, loss=0.0675
Epoch=477, loss=0.0712
Epoch=478, loss=0.0749
Epoch=479, loss=0.0746
Epoch=480, loss=0.0743
Epoch=481, loss=0.0783
Epoch=482, loss=0.0628
Epoch=483, loss=0.0783
Epoch=484, loss=0.0651
Epoch=485, loss=0.0798
Epoch=486, loss=0.0732
Epoch=487, loss=0.0685
Epoch=488, loss=0.0682
Epoch=489, loss=0.0640
Epoch=490, loss=0.0819
Epoch=491, loss=0.0690
Epoch=492, loss=0.0668
Epoch=493, loss=0.0800
Epoch=494, loss=0.0680
Epoch=495, loss=0.0666
Epoch=496, loss=0.0814
Epoch=497, loss=0.0640
Epoch=498, loss=0.0703
Epoch=499, loss=0.0716
Loading 482th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7247+-0.0186, F1Ma=0.6593+-0.0314, acc=0.7247+-0.0186
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7779985586440052, 0.7860561520957581, 0.7843041238041399, 0.7893003326760186, 0.8071428537368774, 0.7039999961853027, 0.7064796686172485, 0.8071428537368774, 0.6919999718666077, 0.7011605501174927, 0.7246793626117373, 0.018570216804658302, 0.6592907006232538, 0.03137648708783388, 0.7246793626117374, 0.018570216804658323]]
=== train DGI model ===
Epoch=000, loss=0.6932
Epoch=001, loss=0.6931
Epoch=002, loss=0.6930
Epoch=003, loss=0.6929
Epoch=004, loss=0.6928
Epoch=005, loss=0.6927
Epoch=006, loss=0.6926
Epoch=007, loss=0.6925
Epoch=008, loss=0.6924
Epoch=009, loss=0.6922
Epoch=010, loss=0.6920
Epoch=011, loss=0.6918
Epoch=012, loss=0.6916
Epoch=013, loss=0.6914
Epoch=014, loss=0.6911
Epoch=015, loss=0.6908
Epoch=016, loss=0.6905
Epoch=017, loss=0.6901
Epoch=018, loss=0.6898
Epoch=019, loss=0.6894
Epoch=020, loss=0.6889
Epoch=021, loss=0.6884
Epoch=022, loss=0.6878
Epoch=023, loss=0.6872
Epoch=024, loss=0.6866
Epoch=025, loss=0.6860
Epoch=026, loss=0.6853
Epoch=027, loss=0.6843
Epoch=028, loss=0.6834
Epoch=029, loss=0.6824
Epoch=030, loss=0.6813
Epoch=031, loss=0.6805
Epoch=032, loss=0.6794
Epoch=033, loss=0.6780
Epoch=034, loss=0.6765
Epoch=035, loss=0.6754
Epoch=036, loss=0.6736
Epoch=037, loss=0.6724
Epoch=038, loss=0.6701
Epoch=039, loss=0.6686
Epoch=040, loss=0.6667
Epoch=041, loss=0.6651
Epoch=042, loss=0.6627
Epoch=043, loss=0.6604
Epoch=044, loss=0.6579
Epoch=045, loss=0.6557
Epoch=046, loss=0.6527
Epoch=047, loss=0.6501
Epoch=048, loss=0.6473
Epoch=049, loss=0.6437
Epoch=050, loss=0.6412
Epoch=051, loss=0.6373
Epoch=052, loss=0.6336
Epoch=053, loss=0.6300
Epoch=054, loss=0.6256
Epoch=055, loss=0.6222
Epoch=056, loss=0.6181
Epoch=057, loss=0.6139
Epoch=058, loss=0.6113
Epoch=059, loss=0.6038
Epoch=060, loss=0.6001
Epoch=061, loss=0.5967
Epoch=062, loss=0.5919
Epoch=063, loss=0.5860
Epoch=064, loss=0.5810
Epoch=065, loss=0.5748
Epoch=066, loss=0.5693
Epoch=067, loss=0.5618
Epoch=068, loss=0.5562
Epoch=069, loss=0.5514
Epoch=070, loss=0.5459
Epoch=071, loss=0.5392
Epoch=072, loss=0.5303
Epoch=073, loss=0.5253
Epoch=074, loss=0.5151
Epoch=075, loss=0.5138
Epoch=076, loss=0.5046
Epoch=077, loss=0.4968
Epoch=078, loss=0.4914
Epoch=079, loss=0.4852
Epoch=080, loss=0.4765
Epoch=081, loss=0.4684
Epoch=082, loss=0.4620
Epoch=083, loss=0.4517
Epoch=084, loss=0.4435
Epoch=085, loss=0.4380
Epoch=086, loss=0.4340
Epoch=087, loss=0.4285
Epoch=088, loss=0.4172
Epoch=089, loss=0.4100
Epoch=090, loss=0.4008
Epoch=091, loss=0.3970
Epoch=092, loss=0.3863
Epoch=093, loss=0.3795
Epoch=094, loss=0.3716
Epoch=095, loss=0.3619
Epoch=096, loss=0.3570
Epoch=097, loss=0.3483
Epoch=098, loss=0.3422
Epoch=099, loss=0.3359
Epoch=100, loss=0.3277
Epoch=101, loss=0.3155
Epoch=102, loss=0.3177
Epoch=103, loss=0.3120
Epoch=104, loss=0.3054
Epoch=105, loss=0.3052
Epoch=106, loss=0.2954
Epoch=107, loss=0.2828
Epoch=108, loss=0.2775
Epoch=109, loss=0.2689
Epoch=110, loss=0.2671
Epoch=111, loss=0.2621
Epoch=112, loss=0.2516
Epoch=113, loss=0.2508
Epoch=114, loss=0.2442
Epoch=115, loss=0.2427
Epoch=116, loss=0.2269
Epoch=117, loss=0.2307
Epoch=118, loss=0.2222
Epoch=119, loss=0.2202
Epoch=120, loss=0.2206
Epoch=121, loss=0.2115
Epoch=122, loss=0.2151
Epoch=123, loss=0.2079
Epoch=124, loss=0.2018
Epoch=125, loss=0.1967
Epoch=126, loss=0.1913
Epoch=127, loss=0.1946
Epoch=128, loss=0.1851
Epoch=129, loss=0.1826
Epoch=130, loss=0.1817
Epoch=131, loss=0.1787
Epoch=132, loss=0.1693
Epoch=133, loss=0.1630
Epoch=134, loss=0.1659
Epoch=135, loss=0.1607
Epoch=136, loss=0.1666
Epoch=137, loss=0.1652
Epoch=138, loss=0.1551
Epoch=139, loss=0.1653
Epoch=140, loss=0.1573
Epoch=141, loss=0.1505
Epoch=142, loss=0.1407
Epoch=143, loss=0.1476
Epoch=144, loss=0.1494
Epoch=145, loss=0.1428
Epoch=146, loss=0.1393
Epoch=147, loss=0.1391
Epoch=148, loss=0.1388
Epoch=149, loss=0.1428
Epoch=150, loss=0.1339
Epoch=151, loss=0.1293
Epoch=152, loss=0.1388
Epoch=153, loss=0.1248
Epoch=154, loss=0.1305
Epoch=155, loss=0.1199
Epoch=156, loss=0.1231
Epoch=157, loss=0.1179
Epoch=158, loss=0.1177
Epoch=159, loss=0.1218
Epoch=160, loss=0.1247
Epoch=161, loss=0.1146
Epoch=162, loss=0.1129
Epoch=163, loss=0.1095
Epoch=164, loss=0.1106
Epoch=165, loss=0.1060
Epoch=166, loss=0.1072
Epoch=167, loss=0.1038
Epoch=168, loss=0.1108
Epoch=169, loss=0.1076
Epoch=170, loss=0.1042
Epoch=171, loss=0.1032
Epoch=172, loss=0.1006
Epoch=173, loss=0.0968
Epoch=174, loss=0.0983
Epoch=175, loss=0.0989
Epoch=176, loss=0.0940
Epoch=177, loss=0.1040
Epoch=178, loss=0.1051
Epoch=179, loss=0.1050
Epoch=180, loss=0.0962
Epoch=181, loss=0.0956
Epoch=182, loss=0.1005
Epoch=183, loss=0.0882
Epoch=184, loss=0.1015
Epoch=185, loss=0.0937
Epoch=186, loss=0.0885
Epoch=187, loss=0.0874
Epoch=188, loss=0.0875
Epoch=189, loss=0.0875
Epoch=190, loss=0.0872
Epoch=191, loss=0.0885
Epoch=192, loss=0.0789
Epoch=193, loss=0.0862
Epoch=194, loss=0.0925
Epoch=195, loss=0.0875
Epoch=196, loss=0.0829
Epoch=197, loss=0.0762
Epoch=198, loss=0.0863
Epoch=199, loss=0.0782
Epoch=200, loss=0.0816
Epoch=201, loss=0.0828
Epoch=202, loss=0.0831
Epoch=203, loss=0.0828
Epoch=204, loss=0.0764
Epoch=205, loss=0.0822
Epoch=206, loss=0.0675
Epoch=207, loss=0.0687
Epoch=208, loss=0.0775
Epoch=209, loss=0.0721
Epoch=210, loss=0.0774
Epoch=211, loss=0.0812
Epoch=212, loss=0.0826
Epoch=213, loss=0.0807
Epoch=214, loss=0.0718
Epoch=215, loss=0.0705
Epoch=216, loss=0.0672
Epoch=217, loss=0.0691
Epoch=218, loss=0.0758
Epoch=219, loss=0.0836
Epoch=220, loss=0.0729
Epoch=221, loss=0.0719
Epoch=222, loss=0.0731
Epoch=223, loss=0.0740
Epoch=224, loss=0.0721
Epoch=225, loss=0.0736
Epoch=226, loss=0.0710
Epoch=227, loss=0.0621
Epoch=228, loss=0.0685
Epoch=229, loss=0.0674
Epoch=230, loss=0.0658
Epoch=231, loss=0.0630
Epoch=232, loss=0.0752
Epoch=233, loss=0.0682
Epoch=234, loss=0.0714
Epoch=235, loss=0.0685
Epoch=236, loss=0.0664
Epoch=237, loss=0.0611
Epoch=238, loss=0.0742
Epoch=239, loss=0.0680
Epoch=240, loss=0.0588
Epoch=241, loss=0.0596
Epoch=242, loss=0.0622
Epoch=243, loss=0.0671
Epoch=244, loss=0.0694
Epoch=245, loss=0.0590
Epoch=246, loss=0.0713
Epoch=247, loss=0.0545
Epoch=248, loss=0.0594
Epoch=249, loss=0.0581
Epoch=250, loss=0.0588
Epoch=251, loss=0.0550
Epoch=252, loss=0.0583
Epoch=253, loss=0.0529
Epoch=254, loss=0.0576
Epoch=255, loss=0.0595
Epoch=256, loss=0.0564
Epoch=257, loss=0.0580
Epoch=258, loss=0.0526
Epoch=259, loss=0.0566
Epoch=260, loss=0.0646
Epoch=261, loss=0.0463
Epoch=262, loss=0.0548
Epoch=263, loss=0.0599
Epoch=264, loss=0.0491
Epoch=265, loss=0.0588
Epoch=266, loss=0.0533
Epoch=267, loss=0.0538
Epoch=268, loss=0.0554
Epoch=269, loss=0.0558
Epoch=270, loss=0.0570
Epoch=271, loss=0.0558
Epoch=272, loss=0.0636
Epoch=273, loss=0.0573
Epoch=274, loss=0.0530
Epoch=275, loss=0.0521
Epoch=276, loss=0.0552
Epoch=277, loss=0.0608
Epoch=278, loss=0.0480
Epoch=279, loss=0.0542
Epoch=280, loss=0.0519
Epoch=281, loss=0.0485
Early stopping!
Loading 261th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7653+-0.0072, F1Ma=0.7308+-0.0107, acc=0.7653+-0.0072
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7779985586440052, 0.7860561520957581, 0.7843041238041399, 0.7893003326760186, 0.8071428537368774, 0.7039999961853027, 0.7064796686172485, 0.8071428537368774, 0.6919999718666077, 0.7011605501174927, 0.7246793626117373, 0.018570216804658302, 0.6592907006232538, 0.03137648708783388, 0.7246793626117374, 0.018570216804658323], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7616003787878789, 0.7770361274775046, 0.7408967014607765, 0.76709149863788, 0.8142856955528259, 0.7400000095367432, 0.7180851101875305, 0.8142856955528259, 0.7360000014305115, 0.7132495045661926, 0.7653322969296541, 0.007222644371748069, 0.7308424410856087, 0.010686700284427964, 0.7653322969296541, 0.0072226443717480346]]
=== train DGI model ===
Epoch=000, loss=0.6931
Epoch=001, loss=0.6930
Epoch=002, loss=0.6928
Epoch=003, loss=0.6926
Epoch=004, loss=0.6923
Epoch=005, loss=0.6920
Epoch=006, loss=0.6916
Epoch=007, loss=0.6911
Epoch=008, loss=0.6907
Epoch=009, loss=0.6901
Epoch=010, loss=0.6894
Epoch=011, loss=0.6886
Epoch=012, loss=0.6876
Epoch=013, loss=0.6864
Epoch=014, loss=0.6854
Epoch=015, loss=0.6841
Epoch=016, loss=0.6825
Epoch=017, loss=0.6807
Epoch=018, loss=0.6789
Epoch=019, loss=0.6767
Epoch=020, loss=0.6740
Epoch=021, loss=0.6717
Epoch=022, loss=0.6688
Epoch=023, loss=0.6658
Epoch=024, loss=0.6621
Epoch=025, loss=0.6586
Epoch=026, loss=0.6547
Epoch=027, loss=0.6499
Epoch=028, loss=0.6455
Epoch=029, loss=0.6403
Epoch=030, loss=0.6344
Epoch=031, loss=0.6288
Epoch=032, loss=0.6222
Epoch=033, loss=0.6164
Epoch=034, loss=0.6077
Epoch=035, loss=0.6007
Epoch=036, loss=0.5942
Epoch=037, loss=0.5843
Epoch=038, loss=0.5756
Epoch=039, loss=0.5665
Epoch=040, loss=0.5554
Epoch=041, loss=0.5455
Epoch=042, loss=0.5376
Epoch=043, loss=0.5242
Epoch=044, loss=0.5148
Epoch=045, loss=0.5059
Epoch=046, loss=0.4956
Epoch=047, loss=0.4773
Epoch=048, loss=0.4666
Epoch=049, loss=0.4592
Epoch=050, loss=0.4439
Epoch=051, loss=0.4254
Epoch=052, loss=0.4185
Epoch=053, loss=0.4045
Epoch=054, loss=0.3887
Epoch=055, loss=0.3843
Epoch=056, loss=0.3684
Epoch=057, loss=0.3578
Epoch=058, loss=0.3435
Epoch=059, loss=0.3311
Epoch=060, loss=0.3235
Epoch=061, loss=0.3124
Epoch=062, loss=0.3012
Epoch=063, loss=0.2851
Epoch=064, loss=0.2785
Epoch=065, loss=0.2646
Epoch=066, loss=0.2637
Epoch=067, loss=0.2514
Epoch=068, loss=0.2394
Epoch=069, loss=0.2289
Epoch=070, loss=0.2141
Epoch=071, loss=0.2156
Epoch=072, loss=0.2126
Epoch=073, loss=0.1914
Epoch=074, loss=0.1918
Epoch=075, loss=0.1895
Epoch=076, loss=0.1869
Epoch=077, loss=0.1721
Epoch=078, loss=0.1806
Epoch=079, loss=0.1668
Epoch=080, loss=0.1668
Epoch=081, loss=0.1568
Epoch=082, loss=0.1519
Epoch=083, loss=0.1430
Epoch=084, loss=0.1406
Epoch=085, loss=0.1365
Epoch=086, loss=0.1373
Epoch=087, loss=0.1333
Epoch=088, loss=0.1262
Epoch=089, loss=0.1230
Epoch=090, loss=0.1221
Epoch=091, loss=0.1263
Epoch=092, loss=0.1133
Epoch=093, loss=0.1190
Epoch=094, loss=0.1113
Epoch=095, loss=0.1095
Epoch=096, loss=0.1200
Epoch=097, loss=0.1081
Epoch=098, loss=0.1056
Epoch=099, loss=0.1104
Epoch=100, loss=0.1104
Epoch=101, loss=0.0900
Epoch=102, loss=0.0928
Epoch=103, loss=0.0938
Epoch=104, loss=0.1028
Epoch=105, loss=0.0860
Epoch=106, loss=0.0937
Epoch=107, loss=0.0904
Epoch=108, loss=0.0801
Epoch=109, loss=0.0875
Epoch=110, loss=0.0847
Epoch=111, loss=0.0904
Epoch=112, loss=0.0841
Epoch=113, loss=0.0759
Epoch=114, loss=0.0813
Epoch=115, loss=0.0857
Epoch=116, loss=0.0806
Epoch=117, loss=0.0893
Epoch=118, loss=0.0783
Epoch=119, loss=0.0796
Epoch=120, loss=0.0887
Epoch=121, loss=0.0761
Epoch=122, loss=0.0784
Epoch=123, loss=0.0769
Epoch=124, loss=0.0797
Epoch=125, loss=0.0676
Epoch=126, loss=0.0822
Epoch=127, loss=0.0738
Epoch=128, loss=0.0775
Epoch=129, loss=0.0695
Epoch=130, loss=0.0682
Epoch=131, loss=0.0683
Epoch=132, loss=0.0626
Epoch=133, loss=0.0631
Epoch=134, loss=0.0691
Epoch=135, loss=0.0653
Epoch=136, loss=0.0581
Epoch=137, loss=0.0631
Epoch=138, loss=0.0634
Epoch=139, loss=0.0672
Epoch=140, loss=0.0652
Epoch=141, loss=0.0664
Epoch=142, loss=0.0651
Epoch=143, loss=0.0657
Epoch=144, loss=0.0659
Epoch=145, loss=0.0536
Epoch=146, loss=0.0600
Epoch=147, loss=0.0548
Epoch=148, loss=0.0569
Epoch=149, loss=0.0582
Epoch=150, loss=0.0625
Epoch=151, loss=0.0631
Epoch=152, loss=0.0583
Epoch=153, loss=0.0601
Epoch=154, loss=0.0539
Epoch=155, loss=0.0529
Epoch=156, loss=0.0529
Epoch=157, loss=0.0542
Epoch=158, loss=0.0605
Epoch=159, loss=0.0596
Epoch=160, loss=0.0620
Epoch=161, loss=0.0543
Epoch=162, loss=0.0601
Epoch=163, loss=0.0562
Epoch=164, loss=0.0463
Epoch=165, loss=0.0560
Epoch=166, loss=0.0658
Epoch=167, loss=0.0485
Epoch=168, loss=0.0542
Epoch=169, loss=0.0582
Epoch=170, loss=0.0588
Epoch=171, loss=0.0483
Epoch=172, loss=0.0594
Epoch=173, loss=0.0549
Epoch=174, loss=0.0554
Epoch=175, loss=0.0605
Epoch=176, loss=0.0560
Epoch=177, loss=0.0545
Epoch=178, loss=0.0533
Epoch=179, loss=0.0527
Epoch=180, loss=0.0408
Epoch=181, loss=0.0503
Epoch=182, loss=0.0598
Epoch=183, loss=0.0448
Epoch=184, loss=0.0381
Epoch=185, loss=0.0474
Epoch=186, loss=0.0451
Epoch=187, loss=0.0406
Epoch=188, loss=0.0506
Epoch=189, loss=0.0567
Epoch=190, loss=0.0443
Epoch=191, loss=0.0423
Epoch=192, loss=0.0428
Epoch=193, loss=0.0626
Epoch=194, loss=0.0446
Epoch=195, loss=0.0508
Epoch=196, loss=0.0455
Epoch=197, loss=0.0550
Epoch=198, loss=0.0465
Epoch=199, loss=0.0442
Epoch=200, loss=0.0431
Epoch=201, loss=0.0367
Epoch=202, loss=0.0492
Epoch=203, loss=0.0422
Epoch=204, loss=0.0471
Epoch=205, loss=0.0375
Epoch=206, loss=0.0440
Epoch=207, loss=0.0351
Epoch=208, loss=0.0436
Epoch=209, loss=0.0490
Epoch=210, loss=0.0397
Epoch=211, loss=0.0417
Epoch=212, loss=0.0440
Epoch=213, loss=0.0472
Epoch=214, loss=0.0499
Epoch=215, loss=0.0429
Epoch=216, loss=0.0396
Epoch=217, loss=0.0372
Epoch=218, loss=0.0309
Epoch=219, loss=0.0432
Epoch=220, loss=0.0462
Epoch=221, loss=0.0374
Epoch=222, loss=0.0503
Epoch=223, loss=0.0501
Epoch=224, loss=0.0418
Epoch=225, loss=0.0365
Epoch=226, loss=0.0338
Epoch=227, loss=0.0385
Epoch=228, loss=0.0425
Epoch=229, loss=0.0431
Epoch=230, loss=0.0395
Epoch=231, loss=0.0335
Epoch=232, loss=0.0331
Epoch=233, loss=0.0399
Epoch=234, loss=0.0332
Epoch=235, loss=0.0454
Epoch=236, loss=0.0429
Epoch=237, loss=0.0410
Epoch=238, loss=0.0311
Early stopping!
Loading 218th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7995+-0.0099, F1Ma=0.7817+-0.0141, acc=0.7995+-0.0099
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7779985586440052, 0.7860561520957581, 0.7843041238041399, 0.7893003326760186, 0.8071428537368774, 0.7039999961853027, 0.7064796686172485, 0.8071428537368774, 0.6919999718666077, 0.7011605501174927, 0.7246793626117373, 0.018570216804658302, 0.6592907006232538, 0.03137648708783388, 0.7246793626117374, 0.018570216804658323], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7616003787878789, 0.7770361274775046, 0.7408967014607765, 0.76709149863788, 0.8142856955528259, 0.7400000095367432, 0.7180851101875305, 0.8142856955528259, 0.7360000014305115, 0.7132495045661926, 0.7653322969296541, 0.007222644371748069, 0.7308424410856087, 0.010686700284427964, 0.7653322969296541, 0.0072226443717480346], ['DGI', 'heat', 64, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7825766214361292, 0.7753805543776502, 0.7765825678989231, 0.7671463592097592, 0.8857142925262451, 0.7860000133514404, 0.7765957713127136, 0.8857142925262451, 0.7900000214576721, 0.7794970870018005, 0.7995336183443451, 0.009949475320637368, 0.7816936831847079, 0.014112545602931879, 0.7995336183443451, 0.009949475320637408]]
=== train DGI model ===
Epoch=000, loss=0.6932
Epoch=001, loss=0.6928
Epoch=002, loss=0.6922
Epoch=003, loss=0.6914
Epoch=004, loss=0.6904
Epoch=005, loss=0.6891
Epoch=006, loss=0.6873
Epoch=007, loss=0.6852
Epoch=008, loss=0.6828
Epoch=009, loss=0.6796
Epoch=010, loss=0.6756
Epoch=011, loss=0.6716
Epoch=012, loss=0.6670
Epoch=013, loss=0.6615
Epoch=014, loss=0.6549
Epoch=015, loss=0.6462
Epoch=016, loss=0.6386
Epoch=017, loss=0.6286
Epoch=018, loss=0.6177
Epoch=019, loss=0.6049
Epoch=020, loss=0.5943
Epoch=021, loss=0.5796
Epoch=022, loss=0.5645
Epoch=023, loss=0.5493
Epoch=024, loss=0.5338
Epoch=025, loss=0.5141
Epoch=026, loss=0.4969
Epoch=027, loss=0.4717
Epoch=028, loss=0.4601
Epoch=029, loss=0.4396
Epoch=030, loss=0.4163
Epoch=031, loss=0.3903
Epoch=032, loss=0.3740
Epoch=033, loss=0.3521
Epoch=034, loss=0.3326
Epoch=035, loss=0.3159
Epoch=036, loss=0.2940
Epoch=037, loss=0.2720
Epoch=038, loss=0.2690
Epoch=039, loss=0.2419
Epoch=040, loss=0.2283
Epoch=041, loss=0.2181
Epoch=042, loss=0.2030
Epoch=043, loss=0.1907
Epoch=044, loss=0.1867
Epoch=045, loss=0.1808
Epoch=046, loss=0.1722
Epoch=047, loss=0.1602
Epoch=048, loss=0.1445
Epoch=049, loss=0.1508
Epoch=050, loss=0.1359
Epoch=051, loss=0.1318
Epoch=052, loss=0.1225
Epoch=053, loss=0.1265
Epoch=054, loss=0.1293
Epoch=055, loss=0.1159
Epoch=056, loss=0.1060
Epoch=057, loss=0.1107
Epoch=058, loss=0.1095
Epoch=059, loss=0.0965
Epoch=060, loss=0.0922
Epoch=061, loss=0.0923
Epoch=062, loss=0.0954
Epoch=063, loss=0.0853
Epoch=064, loss=0.0864
Epoch=065, loss=0.0824
Epoch=066, loss=0.0822
Epoch=067, loss=0.0757
Epoch=068, loss=0.0833
Epoch=069, loss=0.0805
Epoch=070, loss=0.0746
Epoch=071, loss=0.0678
Epoch=072, loss=0.0746
Epoch=073, loss=0.0740
Epoch=074, loss=0.0727
Epoch=075, loss=0.0649
Epoch=076, loss=0.0762
Epoch=077, loss=0.0759
Epoch=078, loss=0.0761
Epoch=079, loss=0.0740
Epoch=080, loss=0.0699
Epoch=081, loss=0.0717
Epoch=082, loss=0.0710
Epoch=083, loss=0.0719
Epoch=084, loss=0.0663
Epoch=085, loss=0.0756
Epoch=086, loss=0.0674
Epoch=087, loss=0.0636
Epoch=088, loss=0.0676
Epoch=089, loss=0.0654
Epoch=090, loss=0.0463
Epoch=091, loss=0.0703
Epoch=092, loss=0.0641
Epoch=093, loss=0.0695
Epoch=094, loss=0.0680
Epoch=095, loss=0.0732
Epoch=096, loss=0.0527
Epoch=097, loss=0.0612
Epoch=098, loss=0.0609
Epoch=099, loss=0.0541
Epoch=100, loss=0.0507
Epoch=101, loss=0.0563
Epoch=102, loss=0.0589
Epoch=103, loss=0.0547
Epoch=104, loss=0.0449
Epoch=105, loss=0.0556
Epoch=106, loss=0.0512
Epoch=107, loss=0.0559
Epoch=108, loss=0.0525
Epoch=109, loss=0.0478
Epoch=110, loss=0.0519
Epoch=111, loss=0.0536
Epoch=112, loss=0.0468
Epoch=113, loss=0.0408
Epoch=114, loss=0.0497
Epoch=115, loss=0.0484
Epoch=116, loss=0.0394
Epoch=117, loss=0.0393
Epoch=118, loss=0.0519
Epoch=119, loss=0.0396
Epoch=120, loss=0.0441
Epoch=121, loss=0.0441
Epoch=122, loss=0.0544
Epoch=123, loss=0.0491
Epoch=124, loss=0.0427
Epoch=125, loss=0.0463
Epoch=126, loss=0.0436
Epoch=127, loss=0.0424
Epoch=128, loss=0.0460
Epoch=129, loss=0.0500
Epoch=130, loss=0.0466
Epoch=131, loss=0.0334
Epoch=132, loss=0.0558
Epoch=133, loss=0.0481
Epoch=134, loss=0.0437
Epoch=135, loss=0.0371
Epoch=136, loss=0.0519
Epoch=137, loss=0.0388
Epoch=138, loss=0.0508
Epoch=139, loss=0.0324
Epoch=140, loss=0.0381
Epoch=141, loss=0.0408
Epoch=142, loss=0.0483
Epoch=143, loss=0.0581
Epoch=144, loss=0.0472
Epoch=145, loss=0.0415
Epoch=146, loss=0.0368
Epoch=147, loss=0.0351
Epoch=148, loss=0.0413
Epoch=149, loss=0.0320
Epoch=150, loss=0.0453
Epoch=151, loss=0.0354
Epoch=152, loss=0.0462
Epoch=153, loss=0.0407
Epoch=154, loss=0.0415
Epoch=155, loss=0.0385
Epoch=156, loss=0.0401
Epoch=157, loss=0.0358
Epoch=158, loss=0.0371
Epoch=159, loss=0.0287
Epoch=160, loss=0.0299
Epoch=161, loss=0.0322
Epoch=162, loss=0.0348
Epoch=163, loss=0.0283
Epoch=164, loss=0.0255
Epoch=165, loss=0.0468
Epoch=166, loss=0.0290
Epoch=167, loss=0.0343
Epoch=168, loss=0.0389
Epoch=169, loss=0.0376
Epoch=170, loss=0.0343
Epoch=171, loss=0.0326
Epoch=172, loss=0.0329
Epoch=173, loss=0.0292
Epoch=174, loss=0.0315
Epoch=175, loss=0.0251
Epoch=176, loss=0.0278
Epoch=177, loss=0.0254
Epoch=178, loss=0.0392
Epoch=179, loss=0.0306
Epoch=180, loss=0.0247
Epoch=181, loss=0.0361
Epoch=182, loss=0.0358
Epoch=183, loss=0.0345
Epoch=184, loss=0.0309
Epoch=185, loss=0.0317
Epoch=186, loss=0.0193
Epoch=187, loss=0.0353
Epoch=188, loss=0.0320
Epoch=189, loss=0.0302
Epoch=190, loss=0.0235
Epoch=191, loss=0.0293
Epoch=192, loss=0.0317
Epoch=193, loss=0.0262
Epoch=194, loss=0.0281
Epoch=195, loss=0.0241
Epoch=196, loss=0.0350
Epoch=197, loss=0.0328
Epoch=198, loss=0.0294
Epoch=199, loss=0.0265
Epoch=200, loss=0.0260
Epoch=201, loss=0.0305
Epoch=202, loss=0.0219
Epoch=203, loss=0.0286
Epoch=204, loss=0.0233
Epoch=205, loss=0.0219
Epoch=206, loss=0.0255
Early stopping!
Loading 186th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.7969+-0.0064, F1Ma=0.7685+-0.0149, acc=0.7969+-0.0064
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7779985586440052, 0.7860561520957581, 0.7843041238041399, 0.7893003326760186, 0.8071428537368774, 0.7039999961853027, 0.7064796686172485, 0.8071428537368774, 0.6919999718666077, 0.7011605501174927, 0.7246793626117373, 0.018570216804658302, 0.6592907006232538, 0.03137648708783388, 0.7246793626117374, 0.018570216804658323], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7616003787878789, 0.7770361274775046, 0.7408967014607765, 0.76709149863788, 0.8142856955528259, 0.7400000095367432, 0.7180851101875305, 0.8142856955528259, 0.7360000014305115, 0.7132495045661926, 0.7653322969296541, 0.007222644371748069, 0.7308424410856087, 0.010686700284427964, 0.7653322969296541, 0.0072226443717480346], ['DGI', 'heat', 64, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7825766214361292, 0.7753805543776502, 0.7765825678989231, 0.7671463592097592, 0.8857142925262451, 0.7860000133514404, 0.7765957713127136, 0.8857142925262451, 0.7900000214576721, 0.7794970870018005, 0.7995336183443451, 0.009949475320637368, 0.7816936831847079, 0.014112545602931879, 0.7995336183443451, 0.009949475320637408], ['DGI', 'heat', 128, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7857391695740036, 0.7795000428751677, 0.7607721915968444, 0.7652273401175297, 0.8857142925262451, 0.8259999752044678, 0.7761121988296509, 0.9285714030265808, 0.8299999833106995, 0.7799806594848633, 0.7968907889623008, 0.006403201034316071, 0.7684559466175834, 0.01493770335566493, 0.7968907889623008, 0.006403201034316051]]
=== train DGI model ===
Epoch=000, loss=0.6931
Epoch=001, loss=0.6916
Epoch=002, loss=0.6889
Epoch=003, loss=0.6851
Epoch=004, loss=0.6802
Epoch=005, loss=0.6735
Epoch=006, loss=0.6659
Epoch=007, loss=0.6568
Epoch=008, loss=0.6449
Epoch=009, loss=0.6319
Epoch=010, loss=0.6144
Epoch=011, loss=0.5954
Epoch=012, loss=0.5773
Epoch=013, loss=0.5559
Epoch=014, loss=0.5281
Epoch=015, loss=0.5081
Epoch=016, loss=0.4827
Epoch=017, loss=0.4540
Epoch=018, loss=0.4251
Epoch=019, loss=0.4018
Epoch=020, loss=0.3805
Epoch=021, loss=0.3433
Epoch=022, loss=0.3191
Epoch=023, loss=0.2933
Epoch=024, loss=0.2709
Epoch=025, loss=0.2518
Epoch=026, loss=0.2335
Epoch=027, loss=0.2279
Epoch=028, loss=0.2116
Epoch=029, loss=0.1979
Epoch=030, loss=0.1816
Epoch=031, loss=0.1914
Epoch=032, loss=0.1611
Epoch=033, loss=0.1646
Epoch=034, loss=0.1504
Epoch=035, loss=0.1583
Epoch=036, loss=0.1449
Epoch=037, loss=0.1129
Epoch=038, loss=0.1285
Epoch=039, loss=0.1052
Epoch=040, loss=0.1223
Epoch=041, loss=0.1245
Epoch=042, loss=0.1212
Epoch=043, loss=0.1189
Epoch=044, loss=0.1010
Epoch=045, loss=0.1240
Epoch=046, loss=0.0994
Epoch=047, loss=0.1075
Epoch=048, loss=0.0888
Epoch=049, loss=0.0843
Epoch=050, loss=0.0847
Epoch=051, loss=0.0747
Epoch=052, loss=0.0860
Epoch=053, loss=0.0791
Epoch=054, loss=0.1013
Epoch=055, loss=0.0837
Epoch=056, loss=0.0798
Epoch=057, loss=0.0748
Epoch=058, loss=0.0687
Epoch=059, loss=0.0742
Epoch=060, loss=0.0724
Epoch=061, loss=0.0761
Epoch=062, loss=0.0700
Epoch=063, loss=0.0686
Epoch=064, loss=0.0767
Epoch=065, loss=0.0753
Epoch=066, loss=0.0751
Epoch=067, loss=0.0805
Epoch=068, loss=0.0525
Epoch=069, loss=0.0533
Epoch=070, loss=0.0602
Epoch=071, loss=0.0743
Epoch=072, loss=0.0665
Epoch=073, loss=0.0585
Epoch=074, loss=0.0790
Epoch=075, loss=0.0620
Epoch=076, loss=0.0568
Epoch=077, loss=0.0668
Epoch=078, loss=0.0702
Epoch=079, loss=0.0652
Epoch=080, loss=0.0689
Epoch=081, loss=0.0500
Epoch=082, loss=0.0668
Epoch=083, loss=0.0625
Epoch=084, loss=0.0586
Epoch=085, loss=0.0612
Epoch=086, loss=0.0524
Epoch=087, loss=0.0498
Epoch=088, loss=0.0459
Epoch=089, loss=0.0723
Epoch=090, loss=0.0494
Epoch=091, loss=0.0532
Epoch=092, loss=0.0472
Epoch=093, loss=0.0589
Epoch=094, loss=0.0566
Epoch=095, loss=0.0622
Epoch=096, loss=0.0512
Epoch=097, loss=0.0535
Epoch=098, loss=0.0522
Epoch=099, loss=0.0605
Epoch=100, loss=0.0390
Epoch=101, loss=0.0463
Epoch=102, loss=0.0498
Epoch=103, loss=0.0416
Epoch=104, loss=0.0635
Epoch=105, loss=0.0310
Epoch=106, loss=0.0545
Epoch=107, loss=0.0460
Epoch=108, loss=0.0474
Epoch=109, loss=0.0436
Epoch=110, loss=0.0423
Epoch=111, loss=0.0449
Epoch=112, loss=0.0401
Epoch=113, loss=0.0395
Epoch=114, loss=0.0431
Epoch=115, loss=0.0339
Epoch=116, loss=0.0399
Epoch=117, loss=0.0354
Epoch=118, loss=0.0413
Epoch=119, loss=0.0370
Epoch=120, loss=0.0408
Epoch=121, loss=0.0398
Epoch=122, loss=0.0452
Epoch=123, loss=0.0400
Epoch=124, loss=0.0303
Epoch=125, loss=0.0399
Epoch=126, loss=0.0506
Epoch=127, loss=0.0363
Epoch=128, loss=0.0323
Epoch=129, loss=0.0369
Epoch=130, loss=0.0462
Epoch=131, loss=0.0287
Epoch=132, loss=0.0369
Epoch=133, loss=0.0276
Epoch=134, loss=0.0373
Epoch=135, loss=0.0326
Epoch=136, loss=0.0316
Epoch=137, loss=0.0317
Epoch=138, loss=0.0195
Epoch=139, loss=0.0349
Epoch=140, loss=0.0219
Epoch=141, loss=0.0398
Epoch=142, loss=0.0296
Epoch=143, loss=0.0278
Epoch=144, loss=0.0356
Epoch=145, loss=0.0397
Epoch=146, loss=0.0287
Epoch=147, loss=0.0303
Epoch=148, loss=0.0266
Epoch=149, loss=0.0260
Epoch=150, loss=0.0282
Epoch=151, loss=0.0315
Epoch=152, loss=0.0286
Epoch=153, loss=0.0345
Epoch=154, loss=0.0308
Epoch=155, loss=0.0281
Epoch=156, loss=0.0324
Epoch=157, loss=0.0245
Epoch=158, loss=0.0214
Early stopping!
Loading 138th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.8039+-0.0043, F1Ma=0.7834+-0.0123, acc=0.8039+-0.0043
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7779985586440052, 0.7860561520957581, 0.7843041238041399, 0.7893003326760186, 0.8071428537368774, 0.7039999961853027, 0.7064796686172485, 0.8071428537368774, 0.6919999718666077, 0.7011605501174927, 0.7246793626117373, 0.018570216804658302, 0.6592907006232538, 0.03137648708783388, 0.7246793626117374, 0.018570216804658323], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7616003787878789, 0.7770361274775046, 0.7408967014607765, 0.76709149863788, 0.8142856955528259, 0.7400000095367432, 0.7180851101875305, 0.8142856955528259, 0.7360000014305115, 0.7132495045661926, 0.7653322969296541, 0.007222644371748069, 0.7308424410856087, 0.010686700284427964, 0.7653322969296541, 0.0072226443717480346], ['DGI', 'heat', 64, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7825766214361292, 0.7753805543776502, 0.7765825678989231, 0.7671463592097592, 0.8857142925262451, 0.7860000133514404, 0.7765957713127136, 0.8857142925262451, 0.7900000214576721, 0.7794970870018005, 0.7995336183443451, 0.009949475320637368, 0.7816936831847079, 0.014112545602931879, 0.7995336183443451, 0.009949475320637408], ['DGI', 'heat', 128, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7857391695740036, 0.7795000428751677, 0.7607721915968444, 0.7652273401175297, 0.8857142925262451, 0.8259999752044678, 0.7761121988296509, 0.9285714030265808, 0.8299999833106995, 0.7799806594848633, 0.7968907889623008, 0.006403201034316071, 0.7684559466175834, 0.01493770335566493, 0.7968907889623008, 0.006403201034316051], ['DGI', 'heat', 256, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.9227123487946467, 0.9143712211957269, 0.9212361690712889, 0.914385095717559, 0.8857142925262451, 0.8199999928474426, 0.7727272510528564, 0.8999999761581421, 0.8180000185966492, 0.7727272510528564, 0.803886513797124, 0.004318055005777307, 0.7833824825094222, 0.012341114955299083, 0.803886513797124, 0.004318055005777266]]
=== train DGI model ===
Epoch=000, loss=0.6931
Epoch=001, loss=0.6908
Epoch=002, loss=0.6860
Epoch=003, loss=0.6758
Epoch=004, loss=0.6684
Epoch=005, loss=0.6530
Epoch=006, loss=0.6410
Epoch=007, loss=0.6275
Epoch=008, loss=0.6005
Epoch=009, loss=0.5854
Epoch=010, loss=0.5630
Epoch=011, loss=0.5234
Epoch=012, loss=0.5078
Epoch=013, loss=0.4837
Epoch=014, loss=0.4362
Epoch=015, loss=0.3926
Epoch=016, loss=0.3797
Epoch=017, loss=0.3636
Epoch=018, loss=0.3380
Epoch=019, loss=0.2869
Epoch=020, loss=0.2915
Epoch=021, loss=0.2559
Epoch=022, loss=0.2247
Epoch=023, loss=0.2226
Epoch=024, loss=0.1885
Epoch=025, loss=0.1856
Epoch=026, loss=0.1764
Epoch=027, loss=0.1628
Epoch=028, loss=0.1474
Epoch=029, loss=0.1294
Epoch=030, loss=0.1253
Epoch=031, loss=0.1304
Epoch=032, loss=0.1196
Epoch=033, loss=0.1199
Epoch=034, loss=0.0952
Epoch=035, loss=0.1195
Epoch=036, loss=0.0803
Epoch=037, loss=0.0965
Epoch=038, loss=0.0597
Epoch=039, loss=0.0762
Epoch=040, loss=0.0658
Epoch=041, loss=0.0668
Epoch=042, loss=0.0541
Epoch=043, loss=0.0609
Epoch=044, loss=0.0587
Epoch=045, loss=0.0442
Epoch=046, loss=0.0534
Epoch=047, loss=0.0545
Epoch=048, loss=0.0552
Epoch=049, loss=0.0573
Epoch=050, loss=0.0472
Epoch=051, loss=0.0386
Epoch=052, loss=0.0382
Epoch=053, loss=0.0400
Epoch=054, loss=0.0329
Epoch=055, loss=0.0356
Epoch=056, loss=0.0296
Epoch=057, loss=0.0366
Epoch=058, loss=0.0432
Epoch=059, loss=0.0388
Epoch=060, loss=0.0372
Epoch=061, loss=0.0401
Epoch=062, loss=0.0262
Epoch=063, loss=0.0335
Epoch=064, loss=0.0264
Epoch=065, loss=0.0262
Epoch=066, loss=0.0190
Epoch=067, loss=0.0214
Epoch=068, loss=0.0266
Epoch=069, loss=0.0201
Epoch=070, loss=0.0221
Epoch=071, loss=0.0215
Epoch=072, loss=0.0226
Epoch=073, loss=0.0199
Epoch=074, loss=0.0200
Epoch=075, loss=0.0253
Epoch=076, loss=0.0224
Epoch=077, loss=0.0246
Epoch=078, loss=0.0132
Epoch=079, loss=0.0081
Epoch=080, loss=0.0164
Epoch=081, loss=0.0178
Epoch=082, loss=0.0129
Epoch=083, loss=0.0137
Epoch=084, loss=0.0109
Epoch=085, loss=0.0098
Epoch=086, loss=0.0161
Epoch=087, loss=0.0177
Epoch=088, loss=0.0222
Epoch=089, loss=0.0166
Epoch=090, loss=0.0173
Epoch=091, loss=0.0178
Epoch=092, loss=0.0181
Epoch=093, loss=0.0171
Epoch=094, loss=0.0100
Epoch=095, loss=0.0150
Epoch=096, loss=0.0098
Epoch=097, loss=0.0208
Epoch=098, loss=0.0137
Epoch=099, loss=0.0104
Early stopping!
Loading 79th epoch
Early stopping!
Start to test process.
done with the edge prediction
done with the node prediction
done with the second node prediction
(E) | label_classification: F1Mi=0.8028+-0.0129, F1Ma=0.7873+-0.0167, acc=0.8028+-0.0129
[['DGI', 'heat', 16, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7779985586440052, 0.7860561520957581, 0.7843041238041399, 0.7893003326760186, 0.8071428537368774, 0.7039999961853027, 0.7064796686172485, 0.8071428537368774, 0.6919999718666077, 0.7011605501174927, 0.7246793626117373, 0.018570216804658302, 0.6592907006232538, 0.03137648708783388, 0.7246793626117374, 0.018570216804658323], ['DGI', 'heat', 32, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7616003787878789, 0.7770361274775046, 0.7408967014607765, 0.76709149863788, 0.8142856955528259, 0.7400000095367432, 0.7180851101875305, 0.8142856955528259, 0.7360000014305115, 0.7132495045661926, 0.7653322969296541, 0.007222644371748069, 0.7308424410856087, 0.010686700284427964, 0.7653322969296541, 0.0072226443717480346], ['DGI', 'heat', 64, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7825766214361292, 0.7753805543776502, 0.7765825678989231, 0.7671463592097592, 0.8857142925262451, 0.7860000133514404, 0.7765957713127136, 0.8857142925262451, 0.7900000214576721, 0.7794970870018005, 0.7995336183443451, 0.009949475320637368, 0.7816936831847079, 0.014112545602931879, 0.7995336183443451, 0.009949475320637408], ['DGI', 'heat', 128, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.7857391695740036, 0.7795000428751677, 0.7607721915968444, 0.7652273401175297, 0.8857142925262451, 0.8259999752044678, 0.7761121988296509, 0.9285714030265808, 0.8299999833106995, 0.7799806594848633, 0.7968907889623008, 0.006403201034316071, 0.7684559466175834, 0.01493770335566493, 0.7968907889623008, 0.006403201034316051], ['DGI', 'heat', 256, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.9227123487946467, 0.9143712211957269, 0.9212361690712889, 0.914385095717559, 0.8857142925262451, 0.8199999928474426, 0.7727272510528564, 0.8999999761581421, 0.8180000185966492, 0.7727272510528564, 0.803886513797124, 0.004318055005777307, 0.7833824825094222, 0.012341114955299083, 0.803886513797124, 0.004318055005777266], ['DGI', 'heat', 512, 15, 2, 'normalize', 0.1, 0.2, 0.4, 0.001, 0.5, 0.2, 0.5, 0.0001, 512, 512, 0.747026855730949, 0.7765718805543651, 0.7689726315941079, 0.7900323964875963, 0.9357143044471741, 0.8059999942779541, 0.7877175807952881, 0.9357143044471741, 0.8059999942779541, 0.7882011532783508, 0.8027982899339292, 0.01286805878299307, 0.7872681944823926, 0.01666193999529034, 0.8027982899339292, 0.012868058782993056]]
