{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382bcff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import clear_output\n",
    "import math, random, torch, collections, time, networkx as nx, matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numbers import Number\n",
    "import scipy as sc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import sys,os\n",
    "sys.path.append('../')\n",
    "from torch_geometric.transforms import RandomLinkSplit, RandomNodeSplit\n",
    "from io_utils.visualisation import *\n",
    "from train_utils.training import *\n",
    "#from layer2 import GaussianSample\n",
    "\n",
    "\n",
    "#### Creating a synthetic graph framework to test (a) Edge prediction (b) Robustness and (c) Transferability\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset_name = 'Cora'\n",
    "dataset = Planetoid(root='../../data/Planetoid', name=dataset_name, transform=NormalizeFeatures())\n",
    "###\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "\n",
    "\n",
    "###### geometric properties of the dataset\n",
    "\n",
    "nodes = {k:  torch.sum(data.edge_index[0,:] ==k).numpy() for k in np.arange(data.x.shape[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b9ffe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#### Compute the edge weights\n",
    "import copy\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "data.edge_weight = torch.ones(data.edge_index.shape[1])\n",
    "transform = T.GDC(\n",
    "        self_loop_weight=1,\n",
    "        normalization_in='row',\n",
    "        normalization_out='row',\n",
    "        diffusion_kwargs=dict(method='ppr', alpha=0.15),\n",
    "        sparsification_kwargs=dict(method='topk', k=30, dim=0),\n",
    "        exact=True,\n",
    "    )\n",
    "\n",
    "data2 = transform(copy.deepcopy(data))\n",
    "    \n",
    "    \n",
    "# edges2, edge_weights = transform(copy.deepcopy(data)).transition_matrix(edge_index=data.edge_index,\n",
    "#                          edge_weight=torch.ones(data.edge_index.shape[1]),\n",
    "#                          num_nodes=data.num_nodes, normalization= 'row')\n",
    "plt.hist(list(data2.edge_attr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cff990",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "(data2.edge_attr ==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95338ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d4209b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0a5f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "edge_index, edge_weight = add_remaining_self_loops(\n",
    "                data.edge_index, torch.ones((data.edge_index.size(1), )), 1., data.num_nodes)\n",
    "row, col = edge_index[0], edge_index[1]\n",
    "deg = scatter_add(edge_weight, col, dim=0, dim_size=data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531a4ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "new_rows = []\n",
    "new_cols = []\n",
    "weights = []\n",
    "sigmas = []\n",
    "n_eff_neighbours = []\n",
    "MAX_N_NEIGHBOURS = 50\n",
    "for u in range(data.num_nodes):\n",
    "    index = ((newA.row == u) & (newA.data >1e-5))\n",
    "    new_rows+= list(newA.row[index])\n",
    "    new_cols+= list(newA.col[index])\n",
    "    #### want to find the appropriate scaling factor\n",
    "    dist_row = newA.data[index]\n",
    "    func = lambda sigma: k(prob_high_dim(sigma, dist_row))\n",
    "    binary_search_result = sigma_binary_search(func, MAX_N_NEIGHBOURS) #### Maybe we should have a varying number of neighbours here\n",
    "    sigmas += [binary_search_result]\n",
    "    weights += list(prob_high_dim(binary_search_result, dist_row))\n",
    "    n_eff_neighbours += [k(prob_high_dim(binary_search_result, dist_row))]\n",
    "print(np.mean(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7aa7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f75a4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### To compare against the usual weights\n",
    "# from torch_geometric.utils import add_remaining_self_loops\n",
    "# from torch_scatter import scatter_add \n",
    "\n",
    "# add_self_loops=True\n",
    "# fill_value = 1\n",
    "# alpha = 0.5\n",
    "\n",
    "# edge_index0 = data.edge_index\n",
    "# edge_weight = torch.ones((edge_index0.size(1), ))\n",
    "\n",
    "\n",
    "# num_nodes = data.num_nodes\n",
    "# edge_index, edge_weight = add_remaining_self_loops(\n",
    "#                 edge_index0, edge_weight, fill_value, num_nodes)\n",
    "\n",
    "# row, col = edge_index[0], edge_index[1]\n",
    "# deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
    "# deg_inv_sqrt = deg.pow_(-alpha)\n",
    "# deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "# L = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "# #L = deg_inv_sqrt[row] * edge_weight #* deg_inv_sqrt[col]\n",
    "# print(L)\n",
    "# plt.hist(L.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f666744",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from torch_geometric.utils import to_scipy_sparse_matrix, from_scipy_sparse_matrix\n",
    "# L = deg_inv_sqrt[row] * edge_weight\n",
    "# P = to_scipy_sparse_matrix(edge_index, edge_attr=L, num_nodes=data.num_nodes)\n",
    "# A = P + P.T - P.multiply(P.T)\n",
    "# d = from_scipy_sparse_matrix(A)\n",
    "# plt.hist(d[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5410ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def prob_low_dim(Y, YY, a=1., b=1.):\n",
    "    \"\"\"\n",
    "    Compute matrix of probabilities q_ij in low-dimensional space\n",
    "    \"\"\"\n",
    "    inv_distances = torch.power(1 + a * torch.sum(torch.square(Y-YY))**b, -1)\n",
    "    return inv_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c8e00",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "class GCNter(nn.Module): # in_dim, hid_dims, out_dim, normalize=True\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, end='linear',\n",
    "    activation='relu', slope=.1, device='cpu', normalize=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.end = end\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.propagate = APPNP(K=1, alpha=0)\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if isinstance(hidden_dim, Number):\n",
    "            self.hidden_dim = [hidden_dim] * (self.n_layers - 1)\n",
    "        elif isinstance(hidden_dim, list):\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            raise ValueError('Wrong argument type for hidden_dim: {}'.format(hidden_dim))\n",
    "\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = [activation] * (self.n_layers - 1)\n",
    "        elif isinstance(activation, list):\n",
    "            self.hidden_dim = activation\n",
    "        else:\n",
    "            raise ValueError('Wrong argument type for activation: {}'.format(activation))\n",
    "\n",
    "        self._act_f = []\n",
    "        for act in self.activation:\n",
    "            if act == 'lrelu':\n",
    "                self._act_f.append(lambda x: F.leaky_relu(x, negative_slope=slope))\n",
    "            elif act == 'relu':\n",
    "                self._act_f.append(lambda x: torch.nn.ReLU()(x))\n",
    "            elif act == 'xtanh':\n",
    "                self._act_f.append(lambda x: self.xtanh(x, alpha=slope))\n",
    "            elif act == 'sigmoid':\n",
    "                self._act_f.append(F.sigmoid)\n",
    "            elif act == 'none':\n",
    "                self._act_f.append(lambda x: x)\n",
    "            else:\n",
    "                ValueError('Incorrect activation: {}'.format(act))\n",
    "\n",
    "        if self.n_layers == 1:\n",
    "            _fc_list = [nn.Linear(self.input_dim, self.output_dim)]\n",
    "        else:\n",
    "            _fc_list = [nn.Linear(self.input_dim, self.hidden_dim[0])]\n",
    "            for i in range(1, self.n_layers - 1):\n",
    "                _fc_list.append(nn.Linear(self.hidden_dim[i - 1], self.hidden_dim[i]))\n",
    "            _fc_list.append(nn.Linear(self.hidden_dim[self.n_layers - 2], self.output_dim))\n",
    "        self.fc = nn.ModuleList(_fc_list)\n",
    "        self.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def xtanh(x, alpha=.1):\n",
    "        \"\"\"tanh function plus an additional linear term\"\"\"\n",
    "        return x.tanh() + alpha * x\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = x\n",
    "        for c in range(self.n_layers):\n",
    "            if c == self.n_layers - 1:\n",
    "                if self.end == 'linear': \n",
    "                    h = self.fc[c](h)\n",
    "                else:\n",
    "                    h = self.propagate(h, edge_index)\n",
    "            else:\n",
    "                h = self.fc[c](h)\n",
    "                h = F.dropout(h, p=0.5, training=self.training)\n",
    "                h = self.propagate(h, edge_index)\n",
    "                if self.normalize: h = F.normalize(h, p=2, dim=1)\n",
    "                h = self._act_f[c](h)\n",
    "        return h\n",
    "\n",
    "model = GCNter(data.num_features, 32, 32, n_layers=2, end='propagate',\n",
    "                                normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c61e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils import (\n",
    "    add_self_loops,\n",
    "    negative_sampling,\n",
    "    remove_self_loops, to_undirected,\n",
    "     to_dense_adj\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb79fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from umap import UMAP\n",
    "def visualize_umap(out, color, size=30, epoch=None, loss = None):\n",
    "    umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "    z = umap_2d.fit_transform(out.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=size, c=color, cmap=\"Set2\")\n",
    "    if epoch is not None and loss is not None:\n",
    "        plt.xlabel(f'Epoch: {epoch}, Loss: {loss:.4f}', fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def deg(index, num_nodes = None,           \n",
    "        dtype = None):\n",
    "    r\"\"\"Computes the (unweighted) degree of a given one-dimensional index tensor.\n",
    "    Args:\n",
    "    index (LongTensor): Index tensor.\n",
    "    num_nodes (int, optional): The number of nodes, *i.e.*\n",
    "    :obj:`max_val + 1` of :attr:`index`. (default: :obj:`None`)\n",
    "    dtype (:obj:`torch.dtype`, optional): The desired data type of the\n",
    "    returned tensor.\\n\\n    :rtype: :class:`Tensor`\\n    \"\"\"\n",
    "    if index.shape[0] != 1: # modify input \n",
    "        index = index[0] \n",
    "    N = num_nodes\n",
    "    out = torch.zeros((N, ), dtype=dtype, device=index.device)\n",
    "    one = torch.ones((index.size(0), ), dtype=out.dtype, device=out.device)\n",
    "    return out.scatter_add_(0, index, one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d125d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model = GCNter(data.num_features, 32, 32, n_layers=2, end='end',\n",
    "                                normalize=True)\n",
    "### we might not want to normalize.\n",
    "### we might not want to use the dot product as a similarity matrix/\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,\n",
    "                                 weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "from models.baseline_models import LogReg\n",
    "from sklearn.metrics import accuracy_score\n",
    "from train_utils import *\n",
    "from scipy import optimize\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_scipy_sparse_matrix, from_scipy_sparse_matrix\n",
    "from torch_scatter import scatter_add \n",
    "\n",
    "\n",
    "training_rate = 0.85\n",
    "MAX_EPOCH_EVAL = 100\n",
    "val_ratio = (1.0 - training_rate) / 3\n",
    "test_ratio = (1.0 - training_rate) / 3 * 2\n",
    "transform = RandomLinkSplit(num_val=val_ratio, num_test=test_ratio,\n",
    "                                is_undirected=True, split_labels=True)\n",
    "transform_nodes = RandomNodeSplit(split = 'test_rest',\n",
    "                                      num_train_per_class = 20,\n",
    "                                      num_val = 500)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "rand_data = transform_nodes(data)\n",
    "# MIN_DIST = 1e-1  Worked really well\n",
    "MIN_DIST = 1e-1\n",
    "EPS = MIN_DIST\n",
    "\n",
    "edge_index0 = torch.vstack([torch.from_numpy(np.array(new_rows)),\n",
    "                          torch.from_numpy(np.array(new_cols))]).long() #### work with the diffusion data\n",
    "\n",
    "P = to_scipy_sparse_matrix(edge_index0, \n",
    "                           edge_attr=torch.from_numpy(np.array(weights)), \n",
    "                           num_nodes=data.num_nodes)\n",
    "A = P + P.T - P.multiply(P.T)\n",
    "d = from_scipy_sparse_matrix(A)#\n",
    "edge_weights  = d[1]  #0.5 * (1 + d[1])\n",
    "rows, cols = d[0]\n",
    "edge_index = torch.vstack([torch.from_numpy(np.array(rows)),\n",
    "                          torch.from_numpy(np.array(cols))]).long()\n",
    "#edge_weights = torch.from_numpy(np.array(edge_weights))\n",
    "print(edge_weights.shape,len(weights), edge_index.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(edge_weights.numpy())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x = np.linspace(0, 3, 300)\n",
    "\n",
    "def f(x, min_dist):\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        if(x[i] <= min_dist):\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(np.exp(- x[i] + min_dist))\n",
    "    return y\n",
    "\n",
    "dist_low_dim = lambda x, a, b: 1 / (1 + a*x**(2*b))\n",
    "\n",
    "\n",
    "p , _ = optimize.curve_fit(dist_low_dim, x, f(x, MIN_DIST))\n",
    "a = p[0]\n",
    "b = p[1]\n",
    "print(\"Hyperparameters a = \" + str(a) + \" and b = \" + str(b))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(140):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.x, data.edge_index)  ### The GNN is trained on the original data\n",
    "    train_index = np.arange(data.edge_index.shape[1]) #dattorch.randperm(data.edge_index.shape[1])#[:1000]\n",
    "    (row_pos, col_pos), edge_weights_pos = remove_self_loops(edge_index, edge_weights) #### try to reconstruct the network based on the diffusion process\n",
    "    indices = torch.randperm(len(row_pos))[:data.edge_index.shape[1]] ### take a subset, there are too many\n",
    "    (row_pos, col_pos), edge_weights_pos = (row_pos[indices], col_pos[indices]), edge_weights_pos[indices]\n",
    "    diff_norm = torch.sum(torch.square(out[row_pos] - out[col_pos]), 1) + EPS\n",
    "    q =  torch.pow(1.  + a * torch.exp(b * torch.log(diff_norm)), -1)\n",
    "    q = torch.clamp(q, EPS, 1.-EPS)  ### ensure above 0\n",
    "    #### Maybe the as and the bs should be node dependent\n",
    "    edge_weights_pos = (1-EPS) * torch.ones((len(row_pos), 1))\n",
    "    loss =  - torch.mean(edge_weights_pos *  torch.log(q)  )  - torch.mean((1.-edge_weights_pos) * (  torch.log(1.-q)  )) \n",
    "    #print(\"loss pos\", loss)\n",
    "    neg_edge_index = negative_sampling(data2.edge_index, data2.num_nodes, num_neg_samples=1000)\n",
    "    row_neg, col_neg = neg_edge_index[0], neg_edge_index[1]\n",
    "    diff_norm_neg = torch.sum(torch.square(out[row_neg] - out[col_neg]), 1)+ EPS\n",
    "    q_neg = torch.pow(1.  + a * torch.exp(b * torch.log(diff_norm_neg)), -1)\n",
    "    #### This might still not be good and too high dimensional!\n",
    "    q_neg = torch.clamp(q_neg, EPS, 1.-EPS)  ### ensure above 0\n",
    "#     #print(\"q_neg\", torch.max(q_neg),  torch.min(q_neg))\n",
    "    edge_weights_neg = EPS * torch.ones((neg_edge_index.shape[1], 1))\n",
    "    loss +=  torch.mean(EPS * ( - torch.log(q_neg)  ))  + torch.mean((1.-EPS) * ( - torch.log(1.-q_neg)  )) \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    #print(\"weight\", model.fc[0].weight.grad)\n",
    "\n",
    "    print('Epoch={:03d}, loss={:.4f}'.format(epoch, loss.item()))\n",
    "    if epoch % 10 == 0 :\n",
    "                print(\"=== Evaluation ===\")\n",
    "                embeds = out\n",
    "                plt.figure()\n",
    "                visualize_umap(out, data.y.numpy(), size=30, epoch=None, loss = None)\n",
    "                plt.show()\n",
    "                _, res, best_epoch = edge_prediction(embeds.detach(), embeds.shape[1],\n",
    "                                         train_data, test_data, val_data,\n",
    "                                         lr=0.01, wd=1e-4,\n",
    "                                         patience = 30,\n",
    "                                         max_epochs=MAX_EPOCH_EVAL)\n",
    "                val_ap, val_roc, test_ap, test_roc, train_ap, train_roc = res[best_epoch][1], res[best_epoch][2], res[best_epoch][3], res[best_epoch][4], res[best_epoch][5], res[best_epoch][6]\n",
    "                \n",
    "                _, nodes_res, best_epoch = node_prediction(embeds.detach(),\n",
    "                                               dataset.num_classes, data.y,\n",
    "                                               rand_data.train_mask, rand_data.test_mask,\n",
    "                                               rand_data.val_mask,\n",
    "                                               lr=0.01, wd=1e-4,\n",
    "                                               patience = 20,\n",
    "                                               max_epochs=MAX_EPOCH_EVAL)\n",
    "\n",
    "                acc_train, val_train, acc = nodes_res[best_epoch][2], nodes_res[best_epoch][3], nodes_res[best_epoch][4]\n",
    "\n",
    "                _, nodes_res_default, best_epoch = node_prediction(embeds.detach(),\n",
    "                                               dataset.num_classes, data.y,\n",
    "                                               data.train_mask, data.test_mask,\n",
    "                                               data.val_mask,\n",
    "                                               lr=0.05, wd=0,\n",
    "                                               patience = 200,\n",
    "                                               max_epochs=MAX_EPOCH_EVAL)\n",
    "                acc_train_default, acc_val_default, acc_default = nodes_res_default[best_epoch][2], nodes_res_default[best_epoch][3], nodes_res_default[best_epoch][4]\n",
    "                print(['ICA', train_roc, train_ap,\n",
    "                   test_roc, test_ap, acc_train, val_train, acc,\n",
    "                   acc_train_default, acc_val_default, acc_default, epoch,])\n",
    "None;\n",
    "\n",
    "None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa0366c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#### Compute the average radius\n",
    "for u in range(20):\n",
    "    index = np.array([e for e in range(len(diff_norm)) if row_pos[e]==u])\n",
    "    x = diff_norm.detach().numpy()[index]\n",
    "    y = diff_norm_neg.detach().numpy()[index]\n",
    "    w = edge_weights.numpy()[index]\n",
    "    q_ind = q.detach().numpy()[index]\n",
    "    print(np.mean(np.sqrt(x)), np.std(np.sqrt(x)), np.mean(np.sqrt(x))/np.std(np.sqrt(x)))\n",
    "    print(np.sum(np.sqrt(x) * q_ind )/np.sum(q_ind ))\n",
    "    plt.figure()\n",
    "    plt.hist([x, y], label=['neighbours', 'neg samples'])\n",
    "    plt.title('nb neighbours = '+ str(len(x)) + ' and average radius: '+str(np.sum(np.sqrt(x) * w)/np.sum(w)) + ' and average radius q: '+str(np.sum(np.sqrt(x) * q_ind)/np.sum(q_ind)))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}