{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import math, random, torch, collections, time, torch.nn.functional as F, networkx as nx, matplotlib.pyplot as plt, numpy as np\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from IPython.display import clear_output\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../../gnumap/')\n",
    "from models.train_models import *\n",
    "from scipy import optimize\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "from umap_functions import *\n",
    "from simulation_utils import make_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_NEIGHBOURS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "make_roll(c=0.6, v=4, omega=12, n_samples = 2000, n_neighbours = 30,\n",
    "              a = 2, b = 2, scale=0.5, plot=True, features=None,\n",
    "              standardize=True)\n",
    "\"\"\"\n",
    "X: coordinates for swissroll\n",
    "t: underlying beta samples\n",
    "new_data: node features\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(12345)\n",
    "X, t, new_data = make_roll(n_neighbours = N_NEIGHBOURS, scale=0.1, n_samples = 4000, features='coordinates')\n",
    "# new_data is graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = X[:,0]\n",
    "y = X[:,1]\n",
    "z = X[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(x, y, z, c=t, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import umap.umap_ as umap\n",
    "import umap\n",
    "import numpy as np\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "X = np.vstack([np.array(x),np.array(y),np.array(z)]).T\n",
    "A_dist = kneighbors_graph(X, N_NEIGHBOURS, mode='distance', include_self=False)\n",
    "embedding = umap.UMAP(n_components=2, n_neighbors= 10, min_dist= 0.3).fit_transform(X)\n",
    "plt.scatter(*embedding.T, s=10, c=t, alpha=0.5, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of neighbor regulates how much local information that you want to reflect on the embedding spcaces. The smaller the local n_neighbor, the more local your embedding is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Draw reconstructed graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A_dist = kneighbors_graph(X, 5, mode='distance', include_self=False)\n",
    "### Very sensitive to wrong edges\n",
    "plt.figure()\n",
    "nx.draw_networkx(nx.from_scipy_sparse_matrix(A_dist),\n",
    "                 pos={i:[new_data.x[i,0].numpy(),new_data.x[i,1].numpy()] for i in range(new_data.num_nodes)},\n",
    "                 # to see the \"roll\" it should by x, y axis\n",
    "                 node_color=t, cmap='Spectral', with_labels=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "n_neighbors = 3 : small k puts more emphasis on the local structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Define GNUMAP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from graph_utils import *\n",
    "heat_edge, heat_weight = get_weights(new_data, method = 'heat')\n",
    "heat_edge2, heat_weight2 = get_weights(new_data, method = 'heat', beta = 0.5)\n",
    "power_edge, power_weight = get_weights(new_data, method = 'power')\n",
    "lap_edge1, lap_weight1 = get_weights(new_data, method = 'laplacian', alpha = 0.1)\n",
    "lap_edge2, lap_weight2 = get_weights(new_data, method = 'laplacian', alpha = 0.5)\n",
    "lap_edge3, lap_weight3 = get_weights(new_data, method = 'laplacian', alpha = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_data.edge_index, new_data.edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(new_data.edge_weight.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(heat_weight2.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from models.data_augmentation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "high_graph_index, high_graph_weights  = get_weights(new_data, neighbours=15, method = 'laplacian', alpha= 0.5)\n",
    "data_tmp = Data(x=new_data.x, y = new_data.y,\n",
    "                edge_index= high_graph_index,\n",
    "                edge_weight= high_graph_weights)\n",
    "out,_ = random_aug(data_tmp, feat_drop_rate = 0.1, edge_mask_rate = 0.3)\n",
    "target_graph_index, target_graph_weights  = out.edge_index, out.edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_graph_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "import cProfile\n",
    "import os\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import remove_self_loops, negative_sampling\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, to_networkx, from_scipy_sparse_matrix\n",
    "import time\n",
    "from umap_functions import *\n",
    "from graph_utils import *\n",
    "from models.data_augmentation import *\n",
    "\n",
    "def train_gnumap(data, hid_dim, dim, n_layers=2, target=None,\n",
    "                 method = 'laplacian', must_propagate=None,\n",
    "                 norm='normalize', neighbours=15,\n",
    "                 patience=20, epochs=200, lr=1e-3, wd=1e-2,\n",
    "                 min_dist=0.1, name_file=\"1\", subsampling=None,\n",
    "                 alpha = 0.5, spread = 1.0, lambd_corr=1e-2,\n",
    "                 beta = 1., gnn_type: str = 'symmetric',\n",
    "                 power = 3,\n",
    "                 feat_drop_rate = 0.0, edge_mask_rate = 0.0,\n",
    "                 repulsion_strength=None,\n",
    "                 local_connectivity=1,\n",
    "                 device=None, colours=None,\n",
    "                 verbose = False):\n",
    "\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    EPS_0 = data.num_edges/ (data.num_nodes ** 2)\n",
    "    _a, _b = find_ab_params(spread, min_dist) # spread , min_dist given as hyperparameter\n",
    "\n",
    "    #if torch_geometric.utils.is_undirected(data.edge_index):\n",
    "    #    new_edge_index, new_edge_attr = torch_geometric.utils.to_undirected(data.edge_index, data.edge_weight)\n",
    "    #else:\n",
    "    #    new_edge_index, new_edge_attr = data.edge_index, data.edge_weight\n",
    "\n",
    "    if torch_geometric.utils.is_undirected(data.edge_index):\n",
    "        new_edge_index, new_edge_attr = data.edge_index, data.edge_weight\n",
    "    else:\n",
    "        new_edge_index, new_edge_attr = torch_geometric.utils.to_undirected(data.edge_index, data.edge_weight)\n",
    "\n",
    "    #### transform edge index into knn matrix\n",
    "    knn = []\n",
    "    for i in range(data.num_nodes):\n",
    "        knn += [list(np.sort(list(new_edge_attr[(new_edge_index[0]==i) & (new_edge_index[1]!=i )].numpy())))]\n",
    "    knn_dists = pd.DataFrame(knn).fillna(0).values\n",
    "    sigmas, rhos = smooth_knn_dist(\n",
    "        knn_dists,\n",
    "        float(neighbours),\n",
    "        local_connectivity=float(local_connectivity),\n",
    "         )\n",
    "\n",
    "    # Maybe use the distance as the original distribution?\n",
    "    vals = [ np.exp(-(np.max([new_edge_attr.numpy()[i] - rhos[new_edge_index[0,i]], 0])) /\n",
    "                    (sigmas[new_edge_index[0,i]])) for i in range(len(new_edge_attr))]\n",
    "\n",
    "    #print(np.where(vals > 1e5))\n",
    "    rows = new_edge_index[0,:].numpy()\n",
    "    cols = new_edge_index[1,:].numpy()\n",
    "    vals = np.array(vals)\n",
    "    vals[vals<1e-5] = 0\n",
    "\n",
    "    high = []\n",
    "    for i in range(data.num_nodes):\n",
    "        high.append(\n",
    "            np.insert((new_edge_attr[(new_edge_index[0]==i) & (new_edge_index[1]!=i )].numpy())/new_edge_attr[(new_edge_index[0] == i) & (new_edge_index[1] != i)].sum().numpy(),0,0)\n",
    "        )\n",
    "    # highs = np.hstack(high)\n",
    "    highs  = new_edge_attr/data.num_edges\n",
    "    p =[]\n",
    "    for i in range(data.num_nodes):\n",
    "        p.append(\n",
    "            highs[(new_edge_index[0] == i) & (new_edge_index[1] != i)].sum().numpy()\n",
    "        )\n",
    "    eta = data.edge_weight\n",
    "    for i in range(len(data.edge_weight)):\n",
    "        eta[i] = (p[data.edge_index[0,i]]+p[data.edge_index[1,i]])/2*data.x.shape[0]\n",
    "\n",
    "    result = scipy.sparse.coo_matrix(\n",
    "  #      (vals, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "         (highs, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "    )\n",
    "    result.eliminate_zeros()\n",
    "    # target_graph_index, target_graph_weights = from_scipy_sparse_matrix(result)\n",
    "    high_graph_index, high_graph_weights  = get_weights(data, neighbours=neighbours, method = method, alpha= alpha, beta = beta,  power = power)\n",
    "    data_tmp = Data(x=data.x, y = data.y,\n",
    "                    edge_index= high_graph_index,\n",
    "                    edge_weight= high_graph_weights)\n",
    "    out, _ = random_aug(data_tmp, feat_drop_rate = feat_drop_rate, edge_mask_rate = edge_mask_rate)\n",
    "    target_graph_index, target_graph_weights  = out.edge_index, out.edge_weight\n",
    "\n",
    "    #### Prune\n",
    "    EPS = 1e-29 #math.exp(-1.0/(2*_b) * math.log(1.0/_a * (1.0/EPS_0 -1)))\n",
    "    print(\"Epsilon is \" + str(EPS))\n",
    "    print(\"Hyperparameters a = \" + str(_a) + \" and b = \" + str(_b))\n",
    "\n",
    "\n",
    "\n",
    "    model = GNN(data.num_features, hid_dim, dim, n_layers=n_layers,\n",
    "                must_propagate=must_propagate,\n",
    "                norm=norm)\n",
    "    model = model.to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n",
    "                             weight_decay=wd)\n",
    "    new_data = Data(x=out.x, edge_index=target_graph_index, # if feature drops the feature matrix should be updated\n",
    "                    y=data.y, edge_weight=target_graph_weights)\n",
    "    sparsity =  new_data.num_edges/(new_data.num_nodes**2 -new_data.num_nodes)\n",
    "    if repulsion_strength is None:\n",
    "        repulsion_strength = 1.0/sparsity\n",
    "        # we have way more samples that are \"not\" connected(sparsity), so need to give more weight to negative sampling to get balanced results\n",
    "    row_pos, col_pos =  new_data.edge_index\n",
    "    index = (row_pos != col_pos)\n",
    "        # to exclude self-connectivity\n",
    "    edge_weights_pos = new_data.edge_weight#[index]\n",
    "\n",
    "    if target is not None:\n",
    "        edge_weights_pos = fast_intersection(row_pos[index], col_pos[index], edge_weights_pos,\n",
    "                                             target, unknown_dist=1.0, far_dist=5.0)\n",
    "        # p_{ij}\n",
    "    if subsampling is None:\n",
    "        row_neg, col_neg = negative_sampling(new_data.edge_index, num_neg_samples = 5 * new_data.edge_index.shape[1] )\n",
    "        # m = 5\n",
    "        index_neg = (row_neg != col_neg)\n",
    "        # edge_weights_neg = EPS * torch.ones(len(row_neg))\n",
    "        edge_weights_neg = m*torch.ones(len(row_neg))\n",
    "        if target is not None:\n",
    "            edge_weights_neg = fast_intersection(row_neg[index_neg], col_neg[index_neg], edge_weights_neg,\n",
    "                                                 target, unknown_dist=1.0, far_dist=5.0)\n",
    "    best_t=0\n",
    "    cnt_wait = 0\n",
    "    best=1e9\n",
    "    log_sigmoid = torch.nn.LogSigmoid()\n",
    "    edges = [(e[0],e[1]) for _, e in enumerate(data.edge_index.numpy().T)]\n",
    "    for epoch in range(epochs):\n",
    "        tic_epoch = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        tic = time.time()\n",
    "        out = model(data.x.float(), new_data.edge_index) # data.edge_index?\n",
    "        diff_norm = torch.sum(torch.square(out[row_pos[index]] - out[col_pos[index]]), 1)\n",
    "        diff_norm = torch.clip(diff_norm, min=1e-3)\n",
    "        log_q = - torch.log1p(_a *  diff_norm ** _b) # 1/(1+a*d^2b)\n",
    "        # log_q = - torch.log1p(1+ diff_norm)\n",
    "        loss_pos = - torch.mean(edge_weights_pos[index] * log_sigmoid(log_q)) - torch.mean((1. - edge_weights_pos[index]) *  (log_sigmoid(log_q) - log_q ) * repulsion_strength)\n",
    "        # log(q/(q+1))\n",
    "\n",
    "\n",
    "        if subsampling is None:\n",
    "            diff_norm_neg = torch.sum(torch.square(out[row_neg[index_neg]] - out[col_neg[index_neg]]), 1) #+ 1e-3\n",
    "            diff_norm_neg = torch.clip(diff_norm_neg, min=1e-3)\n",
    "            log_q_neg = - torch.log1p(_a *  diff_norm_neg ** _b)\n",
    "            # log_q_neg = - torch.log1p(1+ diff_norm_neg)\n",
    "        else:\n",
    "            row_neg, col_neg = negative_sampling(new_data.edge_index,\n",
    "                                                 num_neg_samples=subsampling)\n",
    "            index_neg = (row_neg != col_neg)\n",
    "            edge_weights_neg = EPS * torch.ones(len(row_neg))\n",
    "            if target is not None:\n",
    "                edge_weights_neg = fast_intersection(row_neg[index_neg],\n",
    "                                                     col_neg[index_neg], edge_weights_neg,\n",
    "                                                     target, unknown_dist=1.0, far_dist=5.0)\n",
    "            diff_norm_neg = torch.sum(torch.square(out[row_neg[index_neg]] - out[col_neg[index_neg]]), 1) #+ 1e-3\n",
    "            diff_norm_neg = torch.clip(diff_norm_neg, min=1e-3)\n",
    "            log_q_neg = - torch.log1p(_a *  diff_norm_neg ** _b)\n",
    "            # log_q_neg = - torch.log1p(1+ diff_norm_neg)\n",
    "        print(\"loss before neg\", loss_pos)\n",
    "        loss_neg = - torch.mean((log_sigmoid(log_q_neg) - log_q_neg ) * repulsion_strength)\n",
    "        print(\"loss after neg\", loss_neg)\n",
    "        ### Add a term to make sure that the features are learned independently\n",
    "        c1 = torch.mm(out.T, out)\n",
    "        c1 = c1 / out.shape[0]\n",
    "        iden = torch.tensor(np.eye(out.shape[1])).to(device)\n",
    "        loss_dec1 = (torch.diag_embed(c1) - c1).pow(2).sum()\n",
    "        loss = loss_pos + loss_neg +  lambd_corr * loss_dec1\n",
    "        print(\"loss corr\", lambd_corr * loss_dec1)\n",
    "        print(\"loss final\", loss)\n",
    "        tic =  time.time()\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=4)\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose is True:\n",
    "            if epoch%10== 0:\n",
    "                u = out.detach().numpy()\n",
    "                plt.figure()\n",
    "                plt.scatter(u[:,0], u[:,1], c = t,\n",
    "                            cmap=\"Spectral\")\n",
    "                plt.show()\n",
    "                print(torch.mm(out.T, out)/ new_data.num_nodes)\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr * (1.0 - (float(epoch) / float(epochs)))\n",
    "\n",
    "        print('Epoch={:03d}, loss={:.4f}, time={:.4f}'.format(epoch, loss.item(),time.time()-tic_epoch))\n",
    "        if loss < best:\n",
    "            best = loss\n",
    "            best_t = epoch\n",
    "            cnt_wait = 0\n",
    "            torch.save(model.state_dict(), os.getcwd()  + '/results/best_gnumap_'\n",
    "                                          + str(method) + '_neigh' + str(neighbours)\n",
    "                                          + '_dim' + str(dim) + '_' + name_file +  '.pkl')\n",
    "        else:\n",
    "            cnt_wait += 1\n",
    "        if cnt_wait == patience and epoch>50:\n",
    "            print('Early stopping at epoch {}!'.format(epoch))\n",
    "            break\n",
    "        #print(\"Time epoch after saving\", time.time()-tic_epoch)\n",
    "    #tracker.stop()\n",
    "    print('Loading {}th epoch'.format(best_t))\n",
    "    model.load_state_dict(torch.load(os.getcwd()  + '/results/best_gnumap_' +\n",
    "                                     str(method) + '_neigh' + str(neighbours)\n",
    "                                     + '_dim' + str(dim) + '_' + name_file + '.pkl'))\n",
    "    return(model,target_graph_index, vals, knn_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Test with various weight augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### (d) Heat diffusion N_neighbours = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#xx = torch.ones((X.shape[0], 10))\n",
    "# new_data2.x =  new_data.x[:,:2] # leave out z-axis\n",
    "model4, target_index,_,_  =  train_gnumap(new_data,\n",
    "                                     target=None, hid_dim=256, dim=2,\n",
    "                                     n_layers=1, must_propagate= [True, True, True, True, True],\n",
    "                                     method = 'laplacian', alpha=0.5,\n",
    "                                     gnn_type='symmetric', repulsion_strength=10.,\n",
    "                                     norm='standardize', neighbours=5,\n",
    "                                     beta=1, patience=20, epochs=1000,\n",
    "                                     lr=1e-2,\n",
    "                                     wd=1e-4,\n",
    "                                     lambd_corr=1.,\n",
    "                                     min_dist=0.001,subsampling=100000)\n",
    "out = model4(new_data.x.float(), new_data.edge_index)\n",
    "plt.figure()\n",
    "plt.hist(out.detach().numpy())\n",
    "plt.show()\n",
    "u = out.detach().numpy()\n",
    "plt.figure()\n",
    "plt.scatter(u[:,0], u[:,1], c = t,\n",
    "            cmap=\"Spectral\")\n",
    "plt.show()\n",
    "print(torch.mm(out.T, out)/ new_data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma, alpha, x, p):\n",
    "    loss = torch.sum(-((1-p)**gamma)*x*torch.log(p)-(log(p)**gamma)*(1-x)*torch.log(1-p))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from repo provided by the author\n",
    "import torch\n",
    "\n",
    "\n",
    "def align_loss(x, y, alpha=2):\n",
    "    return (x - y).norm(p=2, dim=1).pow(alpha).mean()\n",
    "\n",
    "\n",
    "def uniform_loss(x, t=2):\n",
    "    return torch.pdist(x, p=2).pow(2).mul(-t).exp().mean().log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
