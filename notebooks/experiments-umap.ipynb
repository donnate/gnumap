{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490e913",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import math, random, torch, collections, time, torch.nn.functional as F, networkx as nx, matplotlib.pyplot as plt, numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9684db81",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import sys,os\n",
    "sys.path.append('../')\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import torch_geometric as tg\n",
    "import pandas as pd\n",
    "from data_utils import *\n",
    "dataset_name = 'Cora'\n",
    "dataset = Planetoid(root='Planetoid', name=dataset_name, transform=NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "dataset_print(dataset)\n",
    "data_print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd90ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, APPNP\n",
    "class GCNter(nn.Module): # in_dim, hid_dims, out_dim, normalize=True\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, end='linear',\n",
    "    activation='relu', slope=.1, device='cpu', normalize=True):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.end = end\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.propagate = APPNP(K=1, alpha=0)\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        if isinstance(hidden_dim, Number):\n",
    "            self.hidden_dim = [hidden_dim] * (self.n_layers - 1)\n",
    "        elif isinstance(hidden_dim, list):\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            raise ValueError('Wrong argument type for hidden_dim: {}'.format(hidden_dim))\n",
    "\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = [activation] * (self.n_layers - 1)\n",
    "        elif isinstance(activation, list):\n",
    "            self.hidden_dim = activation\n",
    "        else:\n",
    "            raise ValueError('Wrong argument type for activation: {}'.format(activation))\n",
    "\n",
    "        self._act_f = []\n",
    "        for act in self.activation:\n",
    "            if act == 'lrelu':\n",
    "                self._act_f.append(lambda x: F.leaky_relu(x, negative_slope=slope))\n",
    "            elif act == 'relu':\n",
    "                self._act_f.append(lambda x: torch.nn.ReLU()(x))\n",
    "            elif act == 'xtanh':\n",
    "                self._act_f.append(lambda x: self.xtanh(x, alpha=slope))\n",
    "            elif act == 'sigmoid':\n",
    "                self._act_f.append(F.sigmoid)\n",
    "            elif act == 'none':\n",
    "                self._act_f.append(lambda x: x)\n",
    "            else:\n",
    "                ValueError('Incorrect activation: {}'.format(act))\n",
    "\n",
    "        if self.n_layers == 1:\n",
    "            _fc_list = [nn.Linear(self.input_dim, self.output_dim)]\n",
    "        else:\n",
    "            _fc_list = [nn.Linear(self.input_dim, self.hidden_dim[0])]\n",
    "            for i in range(1, self.n_layers - 1):\n",
    "                _fc_list.append(nn.Linear(self.hidden_dim[i - 1], self.hidden_dim[i]))\n",
    "            _fc_list.append(nn.Linear(self.hidden_dim[self.n_layers - 2], self.output_dim))\n",
    "        self.fc = nn.ModuleList(_fc_list)\n",
    "        self.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def xtanh(x, alpha=.1):\n",
    "        \"\"\"tanh function plus an additional linear term\"\"\"\n",
    "        return x.tanh() + alpha * x\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = x\n",
    "        for c in range(self.n_layers):\n",
    "            if c == self.n_layers - 1:\n",
    "                if self.end == 'linear': \n",
    "                    h = self.fc[c](h)\n",
    "                else:\n",
    "                    h = self.propagate(h, edge_index)\n",
    "            else:\n",
    "                h = self.fc[c](h)\n",
    "                h = F.dropout(h, p=0.5, training=self.training)\n",
    "                h = self.propagate(h, edge_index)\n",
    "                if self.normalize: h = F.normalize(h, p=2, dim=1)\n",
    "                h = self._act_f[c](h)\n",
    "        return h\n",
    "\n",
    "model = GCNter(data.num_features, 32, 32, n_layers=2, end='propagate',\n",
    "                                normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bfda1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def prob_high_dim(sigma, dist):\n",
    "    \"\"\"\n",
    "    For each row of Euclidean distance matrix (dist_row) compute\n",
    "    probability in high dimensions (1D array)\n",
    "    \"\"\"\n",
    "    d = dist - np.min(dist); d[d < 0] = 0\n",
    "    return np.exp(- d / sigma)\n",
    "\n",
    "def k(prob):\n",
    "    \"\"\"\n",
    "    Compute n_neighbor = k (scalar) for each 1D array of high-dimensional probability\n",
    "    \"\"\"\n",
    "    return np.power(2, np.sum(prob))\n",
    "\n",
    "\n",
    "def sigma_binary_search(k_of_sigma, fixed_k):\n",
    "    \"\"\"\n",
    "    Solve equation k_of_sigma(sigma) = fixed_k\n",
    "    with respect to sigma by the binary search algorithm\n",
    "    Do we really need this?\n",
    "    \"\"\"\n",
    "    sigma_lower_limit = 0;\n",
    "    sigma_upper_limit = 100;\n",
    "    for i in range(20):\n",
    "        approx_sigma = (sigma_lower_limit + sigma_upper_limit) / 2\n",
    "        if k_of_sigma(approx_sigma) < fixed_k:\n",
    "            sigma_lower_limit = approx_sigma\n",
    "        else:\n",
    "            sigma_upper_limit = approx_sigma\n",
    "        if np.abs(fixed_k - k_of_sigma(approx_sigma)) <= 1e-5:\n",
    "            break\n",
    "    return approx_sigma\n",
    "\n",
    "\n",
    "def prob_low_dim(Y, YY, a=1., b=1.):\n",
    "    \"\"\"\n",
    "    Compute matrix of probabilities q_ij in low-dimensional space\n",
    "    \"\"\"\n",
    "    inv_distances = torch.power(1 + a * torch.sum(torch.square(Y-YY))**b, -1)\n",
    "    return inv_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005554c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Compute the edge weights\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, to_networkx\n",
    "import copy\n",
    "from umap import *\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "data.edge_weight = torch.ones(data.edge_index.shape[1])\n",
    "transform = T.GDC(\n",
    "        self_loop_weight=1.,\n",
    "        normalization_in='row',\n",
    "        normalization_out='row',\n",
    "        diffusion_kwargs=dict(method='ppr', alpha=0.2),\n",
    "        sparsification_kwargs=dict(method='topk', k=30, dim=0),\n",
    "        exact=True,\n",
    "    )\n",
    "\n",
    "data2 = transform(copy.deepcopy(data))\n",
    "\n",
    "newA = to_scipy_sparse_matrix(data2.edge_index, data2.edge_attr) \n",
    "rows = []\n",
    "cols = []\n",
    "weights = []\n",
    "sigmas = []\n",
    "n_eff_neighbours = []\n",
    "N_NEIGHBOURS = 20\n",
    "for u in range(data.num_nodes):\n",
    "    rows+= list(newA.row[newA.row == u])\n",
    "    cols+= list(newA.col[newA.row == u])\n",
    "    #### want to find the appropriate scaling factor\n",
    "    dist_row = newA.data[newA.row == u]\n",
    "    func = lambda sigma: k(prob_high_dim(sigma, dist_row))\n",
    "    binary_search_result = sigma_binary_search(func, N_NEIGHBOURS) #### Maybe we should have a varying number of neighbours here\n",
    "    sigmas += [binary_search_result]\n",
    "    weights += list(prob_high_dim(binary_search_result, dist_row))\n",
    "    n_eff_neighbours += [k(prob_high_dim(binary_search_result, dist_row))]\n",
    "plt.hist(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdddc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(np.array(n_eff_neighbours)[np.array(n_eff_neighbours)<10**3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6139efe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "binary_search_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae78d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = GCNter(data.num_features, 512, 512, n_layers=2, end='prop',\n",
    "                                normalize=True)\n",
    "### we might not want to normalize.\n",
    "### we might not want to use the dot product as a similarity matrix/\n",
    "from torch_geometric.transforms import RandomLinkSplit, RandomNodeSplit\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001,\n",
    "                                 weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from train_utils import *\n",
    "from scipy import optimize\n",
    "from torch_geometric.utils import remove_self_loops, negative_sampling, add_remaining_self_loops\n",
    "\n",
    "from io_utils.visualisation import *\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "from edge_prediction import edge_prediction\n",
    "from node_prediction import node_prediction\n",
    "\n",
    "\n",
    "\n",
    "training_rate = 0.85\n",
    "MAX_EPOCH_EVAL = 100\n",
    "#tracker = CarbonTracker(epochs=MAX_EPOCH_EVAL)\n",
    "val_ratio = (1.0 - training_rate) / 3\n",
    "test_ratio = (1.0 - training_rate) / 3 * 2\n",
    "transform = RandomLinkSplit(num_val=val_ratio, num_test=test_ratio,\n",
    "                                is_undirected=True, split_labels=True)\n",
    "transform_nodes = RandomNodeSplit(split = 'test_rest',\n",
    "                                      num_train_per_class = 20,\n",
    "                                      num_val = 500)\n",
    "train_data, val_data, test_data = transform(data)\n",
    "rand_data = transform_nodes(data)\n",
    "# MIN_DIST = 1e-1  Worked really well\n",
    "MIN_DIST = 1.\n",
    "\n",
    "add_self_loops=True\n",
    "fill_value = 1\n",
    "# alpha = 0.5\n",
    "\n",
    "edge_index0 = data.edge_index\n",
    "edge_weight = torch.ones((edge_index0.size(1), ))\n",
    "\n",
    "\n",
    "# num_nodes = data.num_nodes\n",
    "edge_index, edge_weight = add_remaining_self_loops(\n",
    "                 edge_index0, edge_weight, fill_value, data.num_nodes)\n",
    "\n",
    "row, col = edge_index[0], edge_index[1]\n",
    "# deg = scatter_add(edge_weight, col, dim=0, dim_size=num_nodes)\n",
    "# deg_inv_sqrt = deg.pow_(-alpha)\n",
    "# deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "# L = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "# #L = deg_inv_sqrt[row] * edge_weight #* deg_inv_sqrt[col]\n",
    "\n",
    "# P = to_scipy_sparse_matrix(edge_index, edge_attr=L, num_nodes=data.num_nodes)\n",
    "# A = P + P.T - P.multiply(P.T)\n",
    "# d = from_scipy_sparse_matrix(A)#\n",
    "#edge_weights  = d[1]#0.5 * (1 + d[1])\n",
    "edge_index = torch.vstack([torch.from_numpy(np.array(rows)),\n",
    "                          torch.from_numpy(np.array(cols))]).long()\n",
    "edge_weights = torch.from_numpy(np.array(weights))\n",
    "print(edge_weights )\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(edge_weights.numpy())\n",
    "plt.show()\n",
    "MIN_DIST = 1e-1\n",
    "\n",
    "\n",
    "x = np.linspace(0, 2, 300)\n",
    "\n",
    "def f(x, min_dist):\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        if(x[i] <= min_dist):\n",
    "            y.append(1)\n",
    "        else:\n",
    "            y.append(np.exp(- x[i] + min_dist))\n",
    "    return y\n",
    "\n",
    "dist_low_dim = lambda x, a, b: 1 / (1 + a*x**(2*b))\n",
    "EPS = 1e-3\n",
    "\n",
    "\n",
    "p , _ = optimize.curve_fit(dist_low_dim, x, f(x, MIN_DIST))\n",
    "a = p[0]\n",
    "b = p[1]\n",
    "print(\"Hyperparameters a = \" + str(a) + \" and b = \" + str(b))\n",
    "lbda = 1e-2\n",
    "\n",
    "\n",
    "for epoch in range(400):\n",
    "    #tracker.epoch_start()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    #edge\n",
    "    #print(\"out\", out)\n",
    "    # pdb.set_trace()\n",
    "    #### Evaluate on existing edges\n",
    "    output_activation = torch.nn.Sigmoid()\n",
    "    #logits = output_activation(torch.mm(logits_temp, logits_temp.t()))\n",
    "    #edge = output_activation(torch.mm(logits_temp, logits_temp.t()))\n",
    "    #### Look for min per node\n",
    "    #min_dist = \n",
    "    ##### Subsample training edges\n",
    "    #train_index = torch.randperm(data.edge_index.shape[1])[:5000]\n",
    "    train_index = np.arange(data.edge_index.shape[1]) #dattorch.randperm(data.edge_index.shape[1])#[:1000]\n",
    "    #train_indices = train_index[row[train_index]!=col[train_index]]\n",
    "    (row_pos, col_pos), edge_weights_pos = remove_self_loops(edge_index, edge_weights)\n",
    "    #d = torch.cdist(out[row[train_index]], out[col[train_index]], p=2.0)\n",
    "    #d_min = torch.sort(d, 1)[0][:,1]\n",
    "    #### subsample\n",
    "    indices = torch.randperm(len(row_pos))[:data.edge_index.shape[1]]\n",
    "    (row_pos, col_pos), edge_weights_pos = (row_pos[indices], col_pos[indices]), edge_weights_pos[indices]\n",
    "    diff_norm = torch.sum(torch.square(out[row_pos] - out[col_pos]), 1) + EPS\n",
    "    q =  torch.pow(1.  + a * torch.exp(b * torch.log(diff_norm)), -1)\n",
    "    q = torch.clamp(q, EPS, 1.-EPS)  ### ensure above 0\n",
    "    #### Maybe the as and the bs should be node dependent\n",
    "    #print(\"q_pos\",torch.max(q),  torch.min(q))\n",
    "    #loss = criterion(q, L[val_indices])\n",
    "    edge_weights_pos = (1-EPS) * torch.ones((len(row_pos), 1))\n",
    "    #loss = torch.mean((1-EPS) * torch.ones((len(row_pos), 1)) * (torch.log((1-EPS) * torch.ones((len(row_pos), 1))) - torch.log(q)  ))  +\\\n",
    "    #      torch.mean((EPS) *(torch.ones((len(row_pos), 1))) * (torch.log((EPS) *(torch.ones((len(row_pos), 1)))) - torch.log(1.-q)  )) \n",
    "    #loss = torch.mean(q * (-torch.log(edge_weights_pos) + torch.log(q)  ))  + torch.mean((1.-q) * (-torch.log(1.-edge_weights_pos) + torch.log(1.-q)  )) \n",
    "    #### add loss \n",
    "    loss =  -torch.mean(edge_weights_pos *  torch.log(q)  )  - torch.mean((1.-edge_weights_pos) * (  torch.log(1.-q)  )) \n",
    "    #print(\"loss pos\", loss)\n",
    "    neg_edge_index = negative_sampling(data.edge_index, data.num_nodes, num_neg_samples=1000)\n",
    "    row_neg, col_neg = neg_edge_index[0], neg_edge_index[1]\n",
    "# #     loss += criterion(output_activation(torch.sum(out[neg_edge_index[0]] * out[neg_edge_index[1]], 1)), \n",
    "# #                       torch.zeros((neg_edge_index.shape[1])))\n",
    "    diff_norm = torch.sum(torch.square(out[row_neg] - out[col_neg]), 1)+ EPS\n",
    "    #diff_norm = torch.clamp(diff_norm, EPS, 1.-EPS)  ### ensure above 0\n",
    "#     #print(\"diffnorm neg\", torch.sum(torch.isnan(diff_norm)), torch.max(diff_norm),  torch.min(diff_norm))\n",
    "#     #q_neg = torch.exp(-1.0*torch.log(1.  + a * torch.exp(b * torch.log(diff_norm))))\n",
    "    q_neg = torch.pow(1.  + a * torch.exp(b * torch.log(diff_norm)), -1)\n",
    "    #### This might still not be good and too high dimensional!\n",
    "    q_neg = torch.clamp(q_neg, EPS, 1.-EPS)  ### ensure above 0\n",
    "#     #print(\"q_neg\", torch.max(q_neg),  torch.min(q_neg))\n",
    "    edge_weights_neg = EPS * torch.ones((neg_edge_index.shape[1], 1))\n",
    "    loss +=  torch.mean(EPS * ( - torch.log(q_neg)  ))  + torch.mean((1.-EPS) * ( - torch.log(1.-q_neg)  )) \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    #print(\"weight\", model.fc[0].weight.grad)\n",
    "    #tracker.epoch_end()\n",
    "    print('Epoch={:03d}, loss={:.4f}'.format(epoch, loss.item()))\n",
    "    if epoch % 10 == 0 :\n",
    "                print(\"=== Evaluation ===\")\n",
    "                embeds = out\n",
    "                plt.figure()\n",
    "                visualize_umap(out, data.y.numpy(), size=30, epoch=None, loss = None)\n",
    "                plt.show()\n",
    "                _, res, best_epoch = edge_prediction(embeds.detach(), embeds.shape[1],\n",
    "                                         train_data, test_data, val_data,\n",
    "                                         lr=0.01, wd=1e-4,\n",
    "                                         patience = 30,\n",
    "                                         max_epochs=MAX_EPOCH_EVAL)\n",
    "                val_ap, val_roc, test_ap, test_roc, train_ap, train_roc = res[best_epoch][1], res[best_epoch][2], res[best_epoch][3], res[best_epoch][4], res[best_epoch][5], res[best_epoch][6]\n",
    "                \n",
    "                _, nodes_res, best_epoch = node_prediction(embeds.detach(),\n",
    "                                               dataset.num_classes, data.y,\n",
    "                                               rand_data.train_mask, rand_data.test_mask,\n",
    "                                               rand_data.val_mask,\n",
    "                                               lr=0.01, wd=1e-4,\n",
    "                                               patience = 20,\n",
    "                                               max_epochs=MAX_EPOCH_EVAL)\n",
    "\n",
    "                acc_train, val_train, acc = nodes_res[best_epoch][2], nodes_res[best_epoch][3], nodes_res[best_epoch][4]\n",
    "\n",
    "                _, nodes_res_default, best_epoch = node_prediction(embeds.detach(),\n",
    "                                               dataset.num_classes, data.y,\n",
    "                                               data.train_mask, data.test_mask,\n",
    "                                               data.val_mask,\n",
    "                                               lr=0.05, wd=0,\n",
    "                                               patience = 200,\n",
    "                                               max_epochs=MAX_EPOCH_EVAL)\n",
    "                acc_train_default, acc_val_default, acc_default = nodes_res_default[best_epoch][2], nodes_res_default[best_epoch][3], nodes_res_default[best_epoch][4]\n",
    "                print(['UMAP', train_roc, train_ap,\n",
    "                   test_roc, test_ap, acc_train, val_train, acc,\n",
    "                   acc_train_default, acc_val_default, acc_default, epoch,])\n",
    "None;\n",
    "\n",
    "#tracker.stop()\n",
    "None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e48da5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77e2f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e01561",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
