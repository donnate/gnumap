{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "f35e49eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import math, random, torch, collections, time, torch.nn.functional as F, networkx as nx, matplotlib.pyplot as plt, numpy as np\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from IPython.display import clear_output\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "23c5698c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../../gnumap/')\n",
    "from models.train_models import *\n",
    "from scipy import optimize\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from codecarbon import OfflineEmissionsTracker\n",
    "from umap_functions import *\n",
    "from simulation_utils import make_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_NEIGHBOURS = 5\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[x,y,z], t, new_data = make_roll(n_neighbours = N_NEIGHBOURS, scale=0.1, n_samples = 4000, features='coordinates')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(x, y, z, c=t, cmap='Spectral')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import umap.umap_ as umap\n",
    "import umap\n",
    "import numpy as np\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "X = np.vstack([np.array(x),np.array(y),np.array(z)]).T\n",
    "A_dist = kneighbors_graph(X, N_NEIGHBOURS, mode='distance', include_self=False)\n",
    "embedding = umap.UMAP(n_components=2, n_neighbors= 10, min_dist= 0.3).fit_transform(X)\n",
    "plt.scatter(*embedding.T, s=10, c=t, alpha=0.5, cmap='Spectral')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# more local when n_neighbors is small\n",
    "embedding_20 = umap.UMAP(n_components=2, n_neighbors= 20, min_dist= 0.1).fit_transform(X)\n",
    "plt.scatter(*embedding_20.T, s=10, c=t, alpha=0.5, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54601a21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# more global when n_neighbors is large\n",
    "embedding_200 = umap.UMAP(n_components=2, n_neighbors= 200, min_dist= 0.1).fit_transform(X)\n",
    "plt.scatter(*embedding_200.T, s=10, c=t, alpha=0.5, cmap='Spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e5411",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A_dist[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a405b1b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_data.x # coordinate to node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e56bf4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df2f630",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A_dist = kneighbors_graph(X, 5, mode='distance', include_self=False)\n",
    "### Very sensitive to wrong edges\n",
    "plt.figure()\n",
    "nx.draw_networkx(nx.from_scipy_sparse_matrix(A_dist), \n",
    "                 pos={i:[new_data.x[i,0].numpy(),new_data.x[i,1].numpy()] for i in range(new_data.num_nodes)},\n",
    "                 # to see the \"roll\" it should by x, y axis\n",
    "                 node_color=t, cmap='Spectral', with_labels=False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = train_cca_ssg(new_data, channels=2, hid_dim=256, lambd=1e2,\n",
    "                  n_layers=2, epochs=1000, lr=1e-2,\n",
    "                  fmr=0.2, edr =0.4, name_file=\"test\",\n",
    "                  device=None)\n",
    "plt.hist(model.get_embedding(new_data).numpy())\n",
    "plt.show()\n",
    "plt.figure()\n",
    "out = model.get_embedding(new_data).numpy()\n",
    "u = out\n",
    "plt.scatter(u[:,0], u[:,1], c = t, \n",
    "            cmap=\"Spectral\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Deep Graph Infomax\n",
    "https://github.com/PetarV-/DGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d3524",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First, create \"results\" folder at current working directory\n",
    "model = train_dgi(new_data,512, 2, 2, patience=20,\n",
    "              epochs=200, lr=1e-3, name_file=\"1\")\n",
    "plt.hist(model.get_embedding(new_data).numpy())\n",
    "plt.figure()\n",
    "out = model.get_embedding(new_data).numpy()\n",
    "u = out\n",
    "plt.scatter(u[:,0], u[:,1], c = t, \n",
    "            cmap=\"Spectral\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "from models.aggregation import GAPPNP\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int,\n",
    "                 output_dim: int, n_layers: int,\n",
    "                 activation: str='relu', slope: float=.1,\n",
    "                 device: str='cpu',\n",
    "                 alpha_res: float=0, alpha: float=0.5,\n",
    "                 beta: float=1., gnn_type: str = 'symmetric',\n",
    "                 norm: str='normalize',\n",
    "                 must_propagate=None,\n",
    "                 lambd_corr: float = 0):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.gnn_type = gnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.device = device\n",
    "        self.alpha_res = alpha_res\n",
    "        self.alpha = alpha\n",
    "        self.beta= beta\n",
    "        self.must_propagate = must_propagate\n",
    "        self.propagate = GAPPNP(K=1, alpha_res=self.alpha_res,\n",
    "                                alpha = self.alpha,\n",
    "                                gnn_type=self.gnn_type,\n",
    "                                beta = self.beta)\n",
    "        self.norm = norm\n",
    "        if self.must_propagate is None:\n",
    "            self.must_propagate = [True] * self.n_layers\n",
    "        if isinstance(hidden_dim, Number):\n",
    "            self.hidden_dim = [hidden_dim] * (self.n_layers - 1)\n",
    "        elif isinstance(hidden_dim, list):\n",
    "            self.hidden_dim = hidden_dim\n",
    "        else:\n",
    "            raise ValueError('Wrong argument type for hidden_dim: {}'.format(hidden_dim))\n",
    "\n",
    "        if isinstance(activation, str):\n",
    "            self.activation = [activation] * (self.n_layers - 1)\n",
    "        elif isinstance(activation, list):\n",
    "            self.hidden_dim = activation\n",
    "        else:\n",
    "            raise ValueError('Wrong argument type for activation: {}'.format(activation))\n",
    "\n",
    "        self._act_f = []\n",
    "        for act in self.activation:\n",
    "            if act == 'lrelu':\n",
    "                self._act_f.append(lambda x: F.leaky_relu(x, negative_slope=slope))\n",
    "            elif act == 'relu':\n",
    "                self._act_f.append(lambda x: torch.nn.ReLU()(x))\n",
    "            elif act == 'xtanh':\n",
    "                self._act_f.append(lambda x: self.xtanh(x, alpha=slope))\n",
    "            elif act == 'sigmoid':\n",
    "                self._act_f.append(F.sigmoid)\n",
    "            elif act == 'none':\n",
    "                self._act_f.append(lambda x: x)\n",
    "            else:\n",
    "                ValueError('Incorrect activation: {}'.format(act))\n",
    "\n",
    "        if self.n_layers == 1:\n",
    "            _fc_list = [nn.Linear(self.input_dim, self.output_dim)]\n",
    "        else:\n",
    "            _fc_list = [nn.Linear(self.input_dim, self.hidden_dim[0])]\n",
    "            for i in range(1, self.n_layers - 1):\n",
    "                _fc_list.append(nn.Linear(self.hidden_dim[i - 1], self.hidden_dim[i]))\n",
    "            _fc_list.append(nn.Linear(self.hidden_dim[self.n_layers - 2], self.output_dim))\n",
    "        self.fc = nn.ModuleList(_fc_list)\n",
    "        self.to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def xtanh(x, alpha=.1):\n",
    "        \"\"\"tanh function plus an additional linear term\"\"\"\n",
    "        return x.tanh() + alpha * x\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = x\n",
    "        for c in range(self.n_layers):\n",
    "            if c == self.n_layers - 1:\n",
    "                h = self.fc[c](h)\n",
    "                if self.norm == 'normalize' and c==0:\n",
    "                    h = F.normalize(h, p=2, dim=1)\n",
    "                elif self.norm == 'standardize'and c==0:\n",
    "                    h = (h - h.mean(0)) / h.std(0)\n",
    "                elif self.norm == 'uniform'and c==0:\n",
    "                    h = 1 * (h - h.min()) / (h.max() - h.min())\n",
    "                elif self.norm == 'col_uniform'and c==0:\n",
    "                    h = 1 * (h - h.min(0)[0].reshape([1,-1]))/ (h.max(0)[0].reshape([1,-1])-h.min(0)[0].reshape([1,-1]))\n",
    "\n",
    "            else:\n",
    "                h = self.fc[c](h)\n",
    "                h = F.dropout(h, p=0.5, training=self.training)\n",
    "                if self.must_propagate[c]:\n",
    "                    h = self.propagate(h, edge_index)\n",
    "                if self.norm == 'normalize':\n",
    "                    h = F.normalize(h, p=2, dim=1)\n",
    "                elif self.norm == 'standardize':\n",
    "                    h = (h - h.mean(0)) / h.std(0) #z1 = (h1 - h1.mean(0)) / h1.std(0)\n",
    "                elif self.norm == 'uniform':\n",
    "                    h = 1 * (h - h.min()) / (h.max() - h.min())\n",
    "                elif self.norm == 'col_uniform':\n",
    "                    h = 1 * (h - h.min(0)[0].reshape([1,-1]))/ (h.max(0)[0].reshape([1,-1])-h.min(0)[0].reshape([1,-1]))\n",
    "                h = self._act_f[c](h)\n",
    "        if self.norm == 'standardize_last':\n",
    "            h = (h - h.mean(0)) / h.std(0)\n",
    "        return h\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "b3f502d7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GNUMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb61581",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "densmap reference: https://www.biorxiv.org/content/10.1101/2020.05.12.077776v1.full.pdf\n",
    "$ L^{denseMAP} = CE(P||Q) - \\lambda Corr(r_o^{UMAP}, r_e^{UMAP}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "import cProfile\n",
    "import os\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import remove_self_loops, negative_sampling\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, to_networkx, from_scipy_sparse_matrix\n",
    "import time\n",
    "from umap_functions import *\n",
    "\n",
    "def train_gnumap(data, hid_dim, dim, n_layers=2, target=None,\n",
    "                 method = 'laplacian', must_propagate=None,\n",
    "                 norm='normalize', neighbours=15,\n",
    "                 patience=20, epochs=200, lr=1e-3, wd=1e-2,\n",
    "                 min_dist=0.1, name_file=\"1\", subsampling=None,\n",
    "                 alpha: float=0.5, spread = 1.0, lambd_corr=1e-2,\n",
    "                 beta: float=1., gnn_type: str = 'symmetric',\n",
    "                 repulsion_strength=None,\n",
    "                 local_connectivity=1,\n",
    "                 device=None, colours=None):\n",
    "\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    EPS_0 = data.num_edges/ (data.num_nodes ** 2)\n",
    "    _a, _b = find_ab_params(spread, min_dist) # spread , min_dist given as hyperparameter\n",
    "\n",
    "    #if torch_geometric.utils.is_undirected(data.edge_index):\n",
    "    #    new_edge_index, new_edge_attr = torch_geometric.utils.to_undirected(data.edge_index, data.edge_weight)\n",
    "    #else:\n",
    "    #    new_edge_index, new_edge_attr = data.edge_index, data.edge_weight\n",
    "\n",
    "    if torch_geometric.utils.is_undirected(data.edge_index):\n",
    "        new_edge_index, new_edge_attr = data.edge_index, data.edge_weight\n",
    "    else:\n",
    "        new_edge_index, new_edge_attr = torch_geometric.utils.to_undirected(data.edge_index, data.edge_weight)\n",
    "\n",
    "    #### transform edge index into knn matrix\n",
    "    knn = []\n",
    "    for i in range(data.num_nodes):\n",
    "        knn += [list(np.sort(list(new_edge_attr[(new_edge_index[0]==i) & (new_edge_index[1]!=i )].numpy())))]\n",
    "    knn_dists = pd.DataFrame(knn).fillna(0).values\n",
    "    sigmas, rhos = smooth_knn_dist(\n",
    "            knn_dists,\n",
    "            float(neighbours),\n",
    "            local_connectivity=float(local_connectivity),\n",
    "             )\n",
    "\n",
    "    # Maybe use the distance as the original distribution?\n",
    "    vals = [ np.exp(-(np.max([new_edge_attr.numpy()[i] - rhos[new_edge_index[0,i]], 0])) /\n",
    "                    (sigmas[new_edge_index[0,i]])) for i in range(len(new_edge_attr))]\n",
    "\n",
    "    #print(np.where(vals > 1e5))\n",
    "    rows = new_edge_index[0,:].numpy()\n",
    "    cols = new_edge_index[1,:].numpy()\n",
    "    vals = np.array(vals)\n",
    "    vals[vals<1e-5] = 0\n",
    "\n",
    "    high = []\n",
    "    for i in range(data.num_nodes):\n",
    "        high.append(\n",
    "            np.insert((new_edge_attr[(new_edge_index[0]==i) & (new_edge_index[1]!=i )].numpy())/new_edge_attr[(new_edge_index[0] == i) & (new_edge_index[1] != i)].sum().numpy(),0,0)\n",
    "        )\n",
    "    # highs = np.hstack(high)\n",
    "    highs  = new_edge_attr/data.num_edges\n",
    "    p =[]\n",
    "    for i in range(data.num_nodes):\n",
    "        p.append(\n",
    "            highs[(new_edge_index[0] == i) & (new_edge_index[1] != i)].sum().numpy()\n",
    "        )\n",
    "    eta = data.edge_weight\n",
    "    for i in range(len(data.edge_weight)):\n",
    "        eta[i] = (p[data.edge_index[0,i]]+p[data.edge_index[1,i]])/2*data.x.shape[0]\n",
    "\n",
    "    result = scipy.sparse.coo_matrix(\n",
    "  #      (vals, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "         (highs, (rows, cols)), shape=(X.shape[0], X.shape[0])\n",
    "    )\n",
    "    result.eliminate_zeros()\n",
    "    target_graph_index, target_graph_weights = from_scipy_sparse_matrix(result)\n",
    "\n",
    "\n",
    "\n",
    "    #### Prune\n",
    "    EPS = 1e-29 #math.exp(-1.0/(2*_b) * math.log(1.0/_a * (1.0/EPS_0 -1)))\n",
    "    print(\"Epsilon is \" + str(EPS))\n",
    "    print(\"Hyperparameters a = \" + str(_a) + \" and b = \" + str(_b))\n",
    "\n",
    "\n",
    "\n",
    "    model = GNN(data.num_features, hid_dim, dim, n_layers=n_layers,\n",
    "                must_propagate=must_propagate,\n",
    "                norm=norm)\n",
    "    model = model.to(device)\n",
    "    model.apply(init_weights)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n",
    "                             weight_decay=wd)\n",
    "    new_data = Data(x=data.x, edge_index=target_graph_index,\n",
    "                    y=data.y, edge_weight=target_graph_weights)\n",
    "    sparsity =  new_data.num_edges/(new_data.num_nodes**2 -new_data.num_nodes)\n",
    "    if repulsion_strength is None:\n",
    "        repulsion_strength = 1.0/sparsity\n",
    "        # we have way more samples that are \"not\" connected(sparsity), so need to give more weight to negative sampling to get balanced results\n",
    "    row_pos, col_pos =  new_data.edge_index\n",
    "    index = (row_pos != col_pos)\n",
    "        # to exclude self-connectivity\n",
    "    edge_weights_pos = new_data.edge_weight#[index]\n",
    "\n",
    "    if target is not None:\n",
    "        edge_weights_pos = fast_intersection(row_pos[index], col_pos[index], edge_weights_pos,\n",
    "                                             target, unknown_dist=1.0, far_dist=5.0)\n",
    "        # p_{ij}\n",
    "    if subsampling is None:\n",
    "        row_neg, col_neg = negative_sampling(new_data.edge_index, num_neg_samples = 5 * new_data.edge_index.shape[1] )\n",
    "        # m = 5\n",
    "        index_neg = (row_neg != col_neg)\n",
    "        # edge_weights_neg = EPS * torch.ones(len(row_neg))\n",
    "        edge_weights_neg = m*torch.ones(len(row_neg))\n",
    "        if target is not None:\n",
    "            edge_weights_neg = fast_intersection(row_neg[index_neg], col_neg[index_neg], edge_weights_neg,\n",
    "                                                 target, unknown_dist=1.0, far_dist=5.0)\n",
    "    best_t=0\n",
    "    cnt_wait = 0\n",
    "    best=1e9\n",
    "    log_sigmoid = torch.nn.LogSigmoid()\n",
    "    edges = [(e[0],e[1]) for _, e in enumerate(data.edge_index.numpy().T)]\n",
    "    for epoch in range(epochs):\n",
    "        tic_epoch = time.time()\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        tic = time.time()\n",
    "        out = model(data.x.float(), data.edge_index)\n",
    "        diff_norm = torch.sum(torch.square(out[row_pos[index]] - out[col_pos[index]]), 1)\n",
    "        diff_norm = torch.clip(diff_norm, min=1e-3)\n",
    "        log_q = - torch.log1p(_a *  diff_norm ** _b) # 1/(1+a*d^2b)\n",
    "        # log_q = - torch.log1p(1+ diff_norm)\n",
    "        loss_pos = - torch.mean(edge_weights_pos[index] * log_sigmoid(log_q)) - torch.mean((1. - edge_weights_pos[index]) *  (log_sigmoid(log_q) - log_q ) * repulsion_strength)\n",
    "        # log(q/(q+1))\n",
    "\n",
    "\n",
    "        if subsampling is None:\n",
    "            diff_norm_neg = torch.sum(torch.square(out[row_neg[index_neg]] - out[col_neg[index_neg]]), 1) #+ 1e-3\n",
    "            diff_norm_neg = torch.clip(diff_norm_neg, min=1e-3)\n",
    "            log_q_neg = - torch.log1p(_a *  diff_norm_neg ** _b)\n",
    "            # log_q_neg = - torch.log1p(1+ diff_norm_neg)\n",
    "        else:\n",
    "            row_neg, col_neg = negative_sampling(new_data.edge_index,\n",
    "                                                 num_neg_samples=subsampling)\n",
    "            index_neg = (row_neg != col_neg)\n",
    "            edge_weights_neg = EPS * torch.ones(len(row_neg))\n",
    "            if target is not None:\n",
    "                edge_weights_neg = fast_intersection(row_neg[index_neg],\n",
    "                                                     col_neg[index_neg], edge_weights_neg,\n",
    "                                                     target, unknown_dist=1.0, far_dist=5.0)\n",
    "            diff_norm_neg = torch.sum(torch.square(out[row_neg[index_neg]] - out[col_neg[index_neg]]), 1) #+ 1e-3\n",
    "            diff_norm_neg = torch.clip(diff_norm_neg, min=1e-3)\n",
    "            log_q_neg = - torch.log1p(_a *  diff_norm_neg ** _b)\n",
    "            # log_q_neg = - torch.log1p(1+ diff_norm_neg)\n",
    "        print(\"loss before neg\", loss_pos)\n",
    "        loss_neg = - torch.mean((log_sigmoid(log_q_neg) - log_q_neg ) * repulsion_strength)\n",
    "        print(\"loss after neg\", loss_neg)\n",
    "        ### Add a term to make sure that the features are learned independently\n",
    "        c1 = torch.mm(out.T, out)\n",
    "        c1 = c1 / out.shape[0]\n",
    "        iden = torch.tensor(np.eye(out.shape[1])).to(device)\n",
    "        loss_dec1 = (torch.diag_embed(c1) - c1).pow(2).sum()\n",
    "        loss = loss_pos + loss_neg +  lambd_corr * loss_dec1\n",
    "        print(\"loss corr\", lambd_corr * loss_dec1)\n",
    "        print(\"loss final\", loss)\n",
    "        tic =  time.time()\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=4)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch%10== 0:\n",
    "            u = out.detach().numpy()\n",
    "            plt.figure()\n",
    "            plt.scatter(u[:,0], u[:,1], c = t, \n",
    "                        cmap=\"Spectral\")\n",
    "            plt.show()\n",
    "            print(torch.mm(out.T, out)/ new_data.num_nodes)\n",
    "\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr * (1.0 - (float(epoch) / float(epochs)))\n",
    "\n",
    "        print('Epoch={:03d}, loss={:.4f}, time={:.4f}'.format(epoch, loss.item(),time.time()-tic_epoch))\n",
    "        if loss < best:\n",
    "            best = loss\n",
    "            best_t = epoch\n",
    "            cnt_wait = 0\n",
    "            torch.save(model.state_dict(), os.getcwd()  + '/results/best_gnumap_'\n",
    "                                          + str(method) + '_neigh' + str(neighbours)\n",
    "                                          + '_dim' + str(dim) + '_' + name_file +  '.pkl')\n",
    "        else:\n",
    "            cnt_wait += 1\n",
    "        if cnt_wait == patience and epoch>50:\n",
    "            print('Early stopping at epoch {}!'.format(epoch))\n",
    "            break\n",
    "        #print(\"Time epoch after saving\", time.time()-tic_epoch)\n",
    "    #tracker.stop()\n",
    "    print('Loading {}th epoch'.format(best_t))\n",
    "    model.load_state_dict(torch.load(os.getcwd()  + '/results/best_gnumap_' +\n",
    "                                     str(method) + '_neigh' + str(neighbours)\n",
    "                                     + '_dim' + str(dim) + '_' + name_file + '.pkl'))\n",
    "    return(model,target_graph_index, vals, knn_dists)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model2, target_graph_index, vals, knn_dists=  train_gnumap(new_data, \n",
    "                                     target=None, hid_dim=1054, dim=2, \n",
    "                                     n_layers=3, must_propagate= [True, True, True, True, True],\n",
    "                                     method = 'laplacian', alpha=0.5, \n",
    "                                     gnn_type='symmetric', repulsion_strength=100,\n",
    "                                     norm='uniform', neighbours=5,\n",
    "                                      beta=1, patience=20, epochs=1000,\n",
    "                                      lr=1e-1, \n",
    "                                    wd=1e-4,\n",
    "                                    lambd_corr=1e-4,\n",
    "                                       min_dist=0.01,\n",
    "                       subsampling=100000)\n",
    "out = model2(new_data.x.float(), new_data.edge_index)\n",
    "plt.figure()\n",
    "plt.hist(out.detach().numpy())\n",
    "plt.show()\n",
    "u = out.detach().numpy()\n",
    "plt.figure()\n",
    "plt.scatter(u[:,0], u[:,1], c = t, \n",
    "            cmap=\"Spectral\")\n",
    "plt.show()\n",
    "print(torch.mm(out.T, out)/ new_data.num_nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(vals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_edge_index, new_edge_attr = remove_self_loops(new_data.edge_index, new_data.edge_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_dists\n",
    "sigmas, rhos = smooth_knn_dist(\n",
    "            knn_dists,\n",
    "            float(2),\n",
    "            local_connectivity=float(1.),\n",
    "             )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vals = []\n",
    "for i in range(len(new_edge_attr))[:100]:\n",
    "    vals+= [ np.exp(-(np.max([new_edge_attr.numpy()[i] - rhos[new_edge_index[0,i]], 0])) /\n",
    "                    (sigmas[new_edge_index[0,i]]))]\n",
    "    print([i, np.exp(-(np.max([new_edge_attr.numpy()[i] - rhos[new_edge_index[0,i]], 0])) /\n",
    "                    (sigmas[new_edge_index[0,i]]))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(new_edge_index[:,i], new_edge_attr.numpy()[i] , rhos[new_edge_index[0,i]], sigmas[new_edge_index[0,i]])\n",
    "print(np.max(new_edge_attr.numpy()[i] - rhos[new_edge_index[0,i]], 0)/sigmas[new_edge_index[0,i]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.max([new_edge_attr.numpy()[i] - rhos[new_edge_index[0,i]], 0]\n",
    "      )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.max(new_edge_attr.numpy()[i] - rhos[new_edge_index[0,i]], 0)/sigmas[new_edge_index[0,i]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(rhos)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#xx = torch.ones((X.shape[0], 10))\n",
    "new_data3 = copy.deepcopy(new_data)\n",
    "# new_data2.x =  new_data.x[:,:2] # leave out z-axis\n",
    "model2, target_index =  train_gnumap(new_data3,\n",
    "                                     target=None, hid_dim=256, dim=2, \n",
    "                                     n_layers=1, must_propagate= [True, True, True, True, True],\n",
    "                                     method = 'laplacian', alpha=0.5, \n",
    "                                     gnn_type='symmetric', repulsion_strength=1.,\n",
    "                                     norm='standardize', neighbours=2,\n",
    "                                    beta=1, patience=20, epochs=1000,\n",
    "                                       lr=1e-2, \n",
    "                                       wd=1e-4,\n",
    "                                        lambd_corr=1.,\n",
    "                                       min_dist=0.001,\n",
    "                       subsampling=100000)\n",
    "out = model2(new_data3.x.float(), new_data.edge_index)\n",
    "plt.figure()\n",
    "plt.hist(out.detach().numpy())\n",
    "plt.show()\n",
    "u = out.detach().numpy()\n",
    "plt.figure()\n",
    "plt.scatter(u[:,0], u[:,1], c = t, \n",
    "            cmap=\"Spectral\")\n",
    "plt.show()\n",
    "print(torch.mm(out.T, out)/ new_data.num_nodes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}